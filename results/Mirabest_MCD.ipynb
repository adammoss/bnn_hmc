{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adammoss/bnn_hmc/blob/main/results/Mirabest_MCD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow_datasets as tfds\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "0Qzhlk1lAcYm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples_iter = 200\n",
        "dropout = 0.1"
      ],
      "metadata": {
        "id": "fV0em6JQAGyU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install astro-datasets --upgrade\n",
        "!pip install tensorflow_datasets --upgrade\n",
        "import astro_datasets"
      ],
      "metadata": {
        "id": "UCaVOb8qDuss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df56e280-18a6-4681-8e7a-00ba7c8b011b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: astro-datasets in /usr/local/lib/python3.7/dist-packages (0.0.9)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from astro-datasets) (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from astro-datasets) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from astro-datasets) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from astro-datasets) (4.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->astro-datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->astro-datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->astro-datasets) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (0.26.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (14.0.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (4.1.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (1.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (1.47.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (57.4.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (0.2.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (0.5.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (3.17.3)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (1.14.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->astro-datasets) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->astro-datasets) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (3.2.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->astro-datasets) (0.6.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->astro-datasets) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->astro-datasets) (1.9.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->astro-datasets) (5.9.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->astro-datasets) (0.3.5.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->astro-datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->astro-datasets) (4.64.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->astro-datasets) (1.56.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (5.9.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.3.5.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.10.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (3.17.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (4.64.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.6.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.9.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.24.3)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->tensorflow_datasets) (3.8.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.56.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dataset, splitting into training, validation and testing"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "3QjJ8hJkDc3P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": [
        "dataset_train, info_train = tfds.load(name='mirabest/confident', split='train[:80%]', batch_size=50, as_supervised=True, with_info=True)\n",
        "dataset_valid, info_valid = tfds.load(name='mirabest/confident', split='train[80%:]', batch_size=50, as_supervised=True, with_info=True) \n",
        "dataset_test, info_test = tfds.load(name='mirabest/confident', split='test', batch_size=50, as_supervised=True, with_info=True) "
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "PxcCbkYaDc3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define MCD layer"
      ],
      "metadata": {
        "id": "XaopgRkWW1sP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MonteCarloDropout(tf.keras.layers.Dropout):\n",
        "     def call(self, inputs):\n",
        "         return super().call(inputs, training=True)"
      ],
      "metadata": {
        "id": "UeBngvEAW2Ev"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create model"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "BBxP_9PqDc3T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(6, kernel_size=5, strides=1, padding = 'same', input_shape=(150, 150, 1)),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2,2)),\n",
        "    MonteCarloDropout(dropout),\n",
        "    tf.keras.layers.Conv2D(16, kernel_size=5, strides=1, padding = 'same'),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2,2)),\n",
        "    MonteCarloDropout(dropout),\n",
        "    tf.keras.layers.Conv2D(120, kernel_size=5, strides=1, padding = 'same'),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2,2)),\n",
        "    MonteCarloDropout(dropout),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(84),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    MonteCarloDropout(dropout),\n",
        "    tf.keras.layers.Dense(2, activation='softmax'),\n",
        "    ])"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Njap0BGqDc3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "r47JZ-5iDXuf",
        "outputId": "b3568a52-ba95-4234-ef40-bf33b1a95135",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 150, 150, 6)       156       \n",
            "                                                                 \n",
            " re_lu (ReLU)                (None, 150, 150, 6)       0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 74, 74, 6)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " monte_carlo_dropout (MonteC  (None, 74, 74, 6)        0         \n",
            " arloDropout)                                                    \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 74, 74, 16)        2416      \n",
            "                                                                 \n",
            " re_lu_1 (ReLU)              (None, 74, 74, 16)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 36, 36, 16)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " monte_carlo_dropout_1 (Mont  (None, 36, 36, 16)       0         \n",
            " eCarloDropout)                                                  \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 36, 36, 120)       48120     \n",
            "                                                                 \n",
            " re_lu_2 (ReLU)              (None, 36, 36, 120)       0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 17, 17, 120)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " monte_carlo_dropout_2 (Mont  (None, 17, 17, 120)      0         \n",
            " eCarloDropout)                                                  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 34680)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 84)                2913204   \n",
            "                                                                 \n",
            " re_lu_3 (ReLU)              (None, 84)                0         \n",
            "                                                                 \n",
            " monte_carlo_dropout_3 (Mont  (None, 84)               0         \n",
            " eCarloDropout)                                                  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 170       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,964,066\n",
            "Trainable params: 2,964,066\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "yVQ_MlmaDc3U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "source": [
        "loss=tf.keras.losses.SparseCategoricalCrossentropy()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "6yASCNz9Dc3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimiser and learning rate scheduler"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "o05piryBDc3V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=3)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "tRKUeZ8MDc3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile model"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "poiLvWqlDc3W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "source": [
        "model.compile(optimizer=opt,\n",
        "              loss=loss,\n",
        "              metrics='accuracy')\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7fT8Wre_Dc3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save best weights"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "b0qDJiw4Dc3X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "source": [
        "checkpoint_path = \"mirabest/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                  save_weights_only=False,\n",
        "                                                  monitor='val_loss',\n",
        "                                                  mode='min',\n",
        "                                                  verbose=1,\n",
        "                                                  save_best_only=True)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "u_ycRk-vDc3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "JzEdGL4YDc3X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "12/12 [==============================] - ETA: 0s - loss: 3.9943 - accuracy: 0.4940\n",
            "Epoch 1: val_loss improved from inf to 2.29231, saving model to mirabest/cp.ckpt\n",
            "12/12 [==============================] - 5s 175ms/step - loss: 3.9943 - accuracy: 0.4940 - val_loss: 2.2923 - val_accuracy: 0.5753 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 1.8469 - accuracy: 0.6160\n",
            "Epoch 2: val_loss improved from 2.29231 to 1.43546, saving model to mirabest/cp.ckpt\n",
            "12/12 [==============================] - 1s 129ms/step - loss: 1.7629 - accuracy: 0.6244 - val_loss: 1.4355 - val_accuracy: 0.6507 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 1.2275 - accuracy: 0.7000\n",
            "Epoch 3: val_loss improved from 1.43546 to 0.95861, saving model to mirabest/cp.ckpt\n",
            "12/12 [==============================] - 2s 144ms/step - loss: 1.1595 - accuracy: 0.7084 - val_loss: 0.9586 - val_accuracy: 0.6644 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.8613 - accuracy: 0.7356\n",
            "Epoch 4: val_loss did not improve from 0.95861\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.8387 - accuracy: 0.7358 - val_loss: 1.1769 - val_accuracy: 0.6370 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.5860 - accuracy: 0.7667\n",
            "Epoch 5: val_loss improved from 0.95861 to 0.87486, saving model to mirabest/cp.ckpt\n",
            "12/12 [==============================] - 1s 127ms/step - loss: 0.5923 - accuracy: 0.7684 - val_loss: 0.8749 - val_accuracy: 0.6712 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6038 - accuracy: 0.7667\n",
            "Epoch 6: val_loss improved from 0.87486 to 0.87246, saving model to mirabest/cp.ckpt\n",
            "12/12 [==============================] - 1s 126ms/step - loss: 0.6202 - accuracy: 0.7633 - val_loss: 0.8725 - val_accuracy: 0.6575 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.5391 - accuracy: 0.7489\n",
            "Epoch 7: val_loss improved from 0.87246 to 0.81798, saving model to mirabest/cp.ckpt\n",
            "12/12 [==============================] - 2s 149ms/step - loss: 0.5155 - accuracy: 0.7564 - val_loss: 0.8180 - val_accuracy: 0.6712 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.4901 - accuracy: 0.7844\n",
            "Epoch 8: val_loss improved from 0.81798 to 0.67764, saving model to mirabest/cp.ckpt\n",
            "12/12 [==============================] - 1s 127ms/step - loss: 0.4630 - accuracy: 0.7942 - val_loss: 0.6776 - val_accuracy: 0.7055 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.4143 - accuracy: 0.8111\n",
            "Epoch 9: val_loss did not improve from 0.67764\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.4237 - accuracy: 0.8027 - val_loss: 0.7218 - val_accuracy: 0.6986 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.3135 - accuracy: 0.8689\n",
            "Epoch 10: val_loss did not improve from 0.67764\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.3326 - accuracy: 0.8576 - val_loss: 0.7301 - val_accuracy: 0.7055 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.3105 - accuracy: 0.8844\n",
            "Epoch 11: val_loss did not improve from 0.67764\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.3153 - accuracy: 0.8765 - val_loss: 0.7223 - val_accuracy: 0.6986 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.3700 - accuracy: 0.8289\n",
            "Epoch 12: val_loss did not improve from 0.67764\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.3703 - accuracy: 0.8319 - val_loss: 0.7223 - val_accuracy: 0.6986 - lr: 4.7500e-05\n",
            "Epoch 13/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.3288 - accuracy: 0.8540\n",
            "Epoch 13: val_loss improved from 0.67764 to 0.64688, saving model to mirabest/cp.ckpt\n",
            "12/12 [==============================] - 1s 124ms/step - loss: 0.3425 - accuracy: 0.8491 - val_loss: 0.6469 - val_accuracy: 0.7466 - lr: 4.7500e-05\n",
            "Epoch 14/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.3541 - accuracy: 0.8644\n",
            "Epoch 14: val_loss improved from 0.64688 to 0.60612, saving model to mirabest/cp.ckpt\n",
            "12/12 [==============================] - 1s 127ms/step - loss: 0.3485 - accuracy: 0.8611 - val_loss: 0.6061 - val_accuracy: 0.7466 - lr: 4.7500e-05\n",
            "Epoch 15/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.3150 - accuracy: 0.8711\n",
            "Epoch 15: val_loss did not improve from 0.60612\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.3068 - accuracy: 0.8765 - val_loss: 0.6166 - val_accuracy: 0.7329 - lr: 4.7500e-05\n",
            "Epoch 16/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.2860 - accuracy: 0.8756\n",
            "Epoch 16: val_loss did not improve from 0.60612\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.2943 - accuracy: 0.8714 - val_loss: 0.6538 - val_accuracy: 0.7603 - lr: 4.7500e-05\n",
            "Epoch 17/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.2951 - accuracy: 0.8600\n",
            "Epoch 17: val_loss did not improve from 0.60612\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.3098 - accuracy: 0.8576 - val_loss: 0.6933 - val_accuracy: 0.7192 - lr: 4.7500e-05\n",
            "Epoch 18/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.3080 - accuracy: 0.8778\n",
            "Epoch 18: val_loss did not improve from 0.60612\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.3015 - accuracy: 0.8765 - val_loss: 0.7046 - val_accuracy: 0.7329 - lr: 4.5125e-05\n",
            "Epoch 19/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.2970 - accuracy: 0.8689\n",
            "Epoch 19: val_loss improved from 0.60612 to 0.59835, saving model to mirabest/cp.ckpt\n",
            "12/12 [==============================] - 2s 149ms/step - loss: 0.2775 - accuracy: 0.8765 - val_loss: 0.5983 - val_accuracy: 0.7329 - lr: 4.5125e-05\n",
            "Epoch 20/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.2821 - accuracy: 0.8733\n",
            "Epoch 20: val_loss did not improve from 0.59835\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.2794 - accuracy: 0.8748 - val_loss: 0.7144 - val_accuracy: 0.7740 - lr: 4.5125e-05\n",
            "Epoch 21/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.2748 - accuracy: 0.8756\n",
            "Epoch 21: val_loss did not improve from 0.59835\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.2711 - accuracy: 0.8765 - val_loss: 0.7410 - val_accuracy: 0.6849 - lr: 4.5125e-05\n",
            "Epoch 22/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.2439 - accuracy: 0.9089\n",
            "Epoch 22: val_loss did not improve from 0.59835\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.2458 - accuracy: 0.9022 - val_loss: 0.6349 - val_accuracy: 0.7466 - lr: 4.5125e-05\n",
            "Epoch 23/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.2324 - accuracy: 0.8978\n",
            "Epoch 23: val_loss improved from 0.59835 to 0.52134, saving model to mirabest/cp.ckpt\n",
            "12/12 [==============================] - 1s 129ms/step - loss: 0.2205 - accuracy: 0.9005 - val_loss: 0.5213 - val_accuracy: 0.8151 - lr: 4.2869e-05\n",
            "Epoch 24/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.2162 - accuracy: 0.9000\n",
            "Epoch 24: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.2110 - accuracy: 0.9005 - val_loss: 0.6103 - val_accuracy: 0.7740 - lr: 4.2869e-05\n",
            "Epoch 25/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.1978 - accuracy: 0.9156\n",
            "Epoch 25: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.2122 - accuracy: 0.9108 - val_loss: 0.6919 - val_accuracy: 0.7466 - lr: 4.2869e-05\n",
            "Epoch 26/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.2535 - accuracy: 0.8911\n",
            "Epoch 26: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.2395 - accuracy: 0.9005 - val_loss: 0.6685 - val_accuracy: 0.7671 - lr: 4.2869e-05\n",
            "Epoch 27/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.2200 - accuracy: 0.9156\n",
            "Epoch 27: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.2016 - accuracy: 0.9280 - val_loss: 0.6073 - val_accuracy: 0.7808 - lr: 4.0725e-05\n",
            "Epoch 28/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.2009 - accuracy: 0.9067\n",
            "Epoch 28: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.1888 - accuracy: 0.9142 - val_loss: 0.6850 - val_accuracy: 0.7603 - lr: 4.0725e-05\n",
            "Epoch 29/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.2071 - accuracy: 0.9089\n",
            "Epoch 29: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1970 - accuracy: 0.9142 - val_loss: 0.7466 - val_accuracy: 0.7603 - lr: 4.0725e-05\n",
            "Epoch 30/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.1739 - accuracy: 0.9422\n",
            "Epoch 30: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1809 - accuracy: 0.9400 - val_loss: 0.6124 - val_accuracy: 0.8014 - lr: 3.8689e-05\n",
            "Epoch 31/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.2200 - accuracy: 0.9089\n",
            "Epoch 31: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.2050 - accuracy: 0.9177 - val_loss: 0.7066 - val_accuracy: 0.7740 - lr: 3.8689e-05\n",
            "Epoch 32/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.1900 - accuracy: 0.9111\n",
            "Epoch 32: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1796 - accuracy: 0.9211 - val_loss: 0.7319 - val_accuracy: 0.7740 - lr: 3.8689e-05\n",
            "Epoch 33/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.2088 - accuracy: 0.9044\n",
            "Epoch 33: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.2054 - accuracy: 0.9074 - val_loss: 0.7070 - val_accuracy: 0.7740 - lr: 3.6755e-05\n",
            "Epoch 34/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.2224 - accuracy: 0.9111\n",
            "Epoch 34: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1987 - accuracy: 0.9245 - val_loss: 0.6505 - val_accuracy: 0.7740 - lr: 3.6755e-05\n",
            "Epoch 35/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.1916 - accuracy: 0.9111\n",
            "Epoch 35: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1742 - accuracy: 0.9211 - val_loss: 0.6698 - val_accuracy: 0.7808 - lr: 3.6755e-05\n",
            "Epoch 36/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1947 - accuracy: 0.9140\n",
            "Epoch 36: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1855 - accuracy: 0.9177 - val_loss: 0.6007 - val_accuracy: 0.8014 - lr: 3.4917e-05\n",
            "Epoch 37/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1809 - accuracy: 0.9320\n",
            "Epoch 37: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1709 - accuracy: 0.9383 - val_loss: 0.5687 - val_accuracy: 0.7877 - lr: 3.4917e-05\n",
            "Epoch 38/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1797 - accuracy: 0.9260\n",
            "Epoch 38: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1659 - accuracy: 0.9331 - val_loss: 0.6188 - val_accuracy: 0.7945 - lr: 3.4917e-05\n",
            "Epoch 39/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1633 - accuracy: 0.9300\n",
            "Epoch 39: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1566 - accuracy: 0.9348 - val_loss: 0.7317 - val_accuracy: 0.7808 - lr: 3.3171e-05\n",
            "Epoch 40/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1614 - accuracy: 0.9360\n",
            "Epoch 40: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1600 - accuracy: 0.9400 - val_loss: 0.5835 - val_accuracy: 0.7877 - lr: 3.3171e-05\n",
            "Epoch 41/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1430 - accuracy: 0.9400\n",
            "Epoch 41: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1378 - accuracy: 0.9383 - val_loss: 0.5804 - val_accuracy: 0.8014 - lr: 3.3171e-05\n",
            "Epoch 42/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1489 - accuracy: 0.9480\n",
            "Epoch 42: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1490 - accuracy: 0.9468 - val_loss: 0.5788 - val_accuracy: 0.8082 - lr: 3.1512e-05\n",
            "Epoch 43/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1427 - accuracy: 0.9480\n",
            "Epoch 43: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1417 - accuracy: 0.9468 - val_loss: 0.6024 - val_accuracy: 0.8219 - lr: 3.1512e-05\n",
            "Epoch 44/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1347 - accuracy: 0.9360\n",
            "Epoch 44: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1304 - accuracy: 0.9400 - val_loss: 0.7290 - val_accuracy: 0.7740 - lr: 3.1512e-05\n",
            "Epoch 45/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1438 - accuracy: 0.9300\n",
            "Epoch 45: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1366 - accuracy: 0.9365 - val_loss: 0.7063 - val_accuracy: 0.7740 - lr: 2.9937e-05\n",
            "Epoch 46/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1278 - accuracy: 0.9580\n",
            "Epoch 46: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1215 - accuracy: 0.9623 - val_loss: 0.5742 - val_accuracy: 0.8288 - lr: 2.9937e-05\n",
            "Epoch 47/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1554 - accuracy: 0.9440\n",
            "Epoch 47: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1472 - accuracy: 0.9468 - val_loss: 0.6847 - val_accuracy: 0.8151 - lr: 2.9937e-05\n",
            "Epoch 48/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.1561 - accuracy: 0.9356\n",
            "Epoch 48: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.1468 - accuracy: 0.9400 - val_loss: 0.7990 - val_accuracy: 0.7671 - lr: 2.8440e-05\n",
            "Epoch 49/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1369 - accuracy: 0.9400\n",
            "Epoch 49: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1360 - accuracy: 0.9434 - val_loss: 0.6731 - val_accuracy: 0.8014 - lr: 2.8440e-05\n",
            "Epoch 50/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1197 - accuracy: 0.9480\n",
            "Epoch 50: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1236 - accuracy: 0.9451 - val_loss: 0.8208 - val_accuracy: 0.8151 - lr: 2.8440e-05\n",
            "Epoch 51/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.1239 - accuracy: 0.9533\n",
            "Epoch 51: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1235 - accuracy: 0.9537 - val_loss: 0.6028 - val_accuracy: 0.7945 - lr: 2.7018e-05\n",
            "Epoch 52/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1095 - accuracy: 0.9600\n",
            "Epoch 52: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.1071 - accuracy: 0.9623 - val_loss: 0.7789 - val_accuracy: 0.7877 - lr: 2.7018e-05\n",
            "Epoch 53/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1260 - accuracy: 0.9480\n",
            "Epoch 53: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1155 - accuracy: 0.9537 - val_loss: 0.7022 - val_accuracy: 0.8219 - lr: 2.7018e-05\n",
            "Epoch 54/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1296 - accuracy: 0.9620\n",
            "Epoch 54: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1223 - accuracy: 0.9623 - val_loss: 0.7599 - val_accuracy: 0.7877 - lr: 2.5667e-05\n",
            "Epoch 55/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1099 - accuracy: 0.9600\n",
            "Epoch 55: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.1037 - accuracy: 0.9657 - val_loss: 0.6429 - val_accuracy: 0.8082 - lr: 2.5667e-05\n",
            "Epoch 56/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1347 - accuracy: 0.9500\n",
            "Epoch 56: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1253 - accuracy: 0.9554 - val_loss: 0.7186 - val_accuracy: 0.7671 - lr: 2.5667e-05\n",
            "Epoch 57/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1562 - accuracy: 0.9440\n",
            "Epoch 57: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1518 - accuracy: 0.9417 - val_loss: 0.7887 - val_accuracy: 0.7808 - lr: 2.4384e-05\n",
            "Epoch 58/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0961 - accuracy: 0.9620\n",
            "Epoch 58: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0935 - accuracy: 0.9640 - val_loss: 0.7538 - val_accuracy: 0.7671 - lr: 2.4384e-05\n",
            "Epoch 59/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.1249 - accuracy: 0.9578\n",
            "Epoch 59: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1219 - accuracy: 0.9554 - val_loss: 0.8555 - val_accuracy: 0.8014 - lr: 2.4384e-05\n",
            "Epoch 60/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1233 - accuracy: 0.9400\n",
            "Epoch 60: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1174 - accuracy: 0.9451 - val_loss: 0.6629 - val_accuracy: 0.7877 - lr: 2.3165e-05\n",
            "Epoch 61/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1057 - accuracy: 0.9640\n",
            "Epoch 61: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1056 - accuracy: 0.9640 - val_loss: 0.7301 - val_accuracy: 0.8219 - lr: 2.3165e-05\n",
            "Epoch 62/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0919 - accuracy: 0.9720\n",
            "Epoch 62: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0866 - accuracy: 0.9743 - val_loss: 0.6495 - val_accuracy: 0.7877 - lr: 2.3165e-05\n",
            "Epoch 63/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0953 - accuracy: 0.9560\n",
            "Epoch 63: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0887 - accuracy: 0.9623 - val_loss: 0.7994 - val_accuracy: 0.7671 - lr: 2.2006e-05\n",
            "Epoch 64/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1206 - accuracy: 0.9540\n",
            "Epoch 64: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1218 - accuracy: 0.9520 - val_loss: 0.7068 - val_accuracy: 0.8082 - lr: 2.2006e-05\n",
            "Epoch 65/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1220 - accuracy: 0.9580\n",
            "Epoch 65: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1193 - accuracy: 0.9605 - val_loss: 0.7057 - val_accuracy: 0.7808 - lr: 2.2006e-05\n",
            "Epoch 66/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0955 - accuracy: 0.9600\n",
            "Epoch 66: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0951 - accuracy: 0.9605 - val_loss: 0.7038 - val_accuracy: 0.7877 - lr: 2.0906e-05\n",
            "Epoch 67/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1001 - accuracy: 0.9620\n",
            "Epoch 67: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1023 - accuracy: 0.9605 - val_loss: 0.6799 - val_accuracy: 0.8219 - lr: 2.0906e-05\n",
            "Epoch 68/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1119 - accuracy: 0.9640\n",
            "Epoch 68: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1048 - accuracy: 0.9657 - val_loss: 0.7583 - val_accuracy: 0.7603 - lr: 2.0906e-05\n",
            "Epoch 69/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.1079 - accuracy: 0.9580\n",
            "Epoch 69: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.1006 - accuracy: 0.9623 - val_loss: 0.7761 - val_accuracy: 0.7877 - lr: 1.9861e-05\n",
            "Epoch 70/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0912 - accuracy: 0.9660\n",
            "Epoch 70: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0907 - accuracy: 0.9674 - val_loss: 0.7923 - val_accuracy: 0.7603 - lr: 1.9861e-05\n",
            "Epoch 71/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0929 - accuracy: 0.9660\n",
            "Epoch 71: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0892 - accuracy: 0.9691 - val_loss: 0.6081 - val_accuracy: 0.7877 - lr: 1.9861e-05\n",
            "Epoch 72/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0837 - accuracy: 0.9700\n",
            "Epoch 72: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0803 - accuracy: 0.9726 - val_loss: 0.7851 - val_accuracy: 0.7945 - lr: 1.8868e-05\n",
            "Epoch 73/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0872 - accuracy: 0.9660\n",
            "Epoch 73: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0828 - accuracy: 0.9674 - val_loss: 0.7474 - val_accuracy: 0.8288 - lr: 1.8868e-05\n",
            "Epoch 74/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.0940 - accuracy: 0.9600\n",
            "Epoch 74: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0871 - accuracy: 0.9640 - val_loss: 0.6247 - val_accuracy: 0.7603 - lr: 1.8868e-05\n",
            "Epoch 75/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0675 - accuracy: 0.9800\n",
            "Epoch 75: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0688 - accuracy: 0.9794 - val_loss: 0.7373 - val_accuracy: 0.7877 - lr: 1.7924e-05\n",
            "Epoch 76/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0941 - accuracy: 0.9660\n",
            "Epoch 76: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0927 - accuracy: 0.9674 - val_loss: 0.7745 - val_accuracy: 0.8219 - lr: 1.7924e-05\n",
            "Epoch 77/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0844 - accuracy: 0.9640\n",
            "Epoch 77: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0802 - accuracy: 0.9674 - val_loss: 0.7386 - val_accuracy: 0.7671 - lr: 1.7924e-05\n",
            "Epoch 78/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0919 - accuracy: 0.9760\n",
            "Epoch 78: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0926 - accuracy: 0.9726 - val_loss: 0.7883 - val_accuracy: 0.7945 - lr: 1.7028e-05\n",
            "Epoch 79/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0928 - accuracy: 0.9580\n",
            "Epoch 79: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0905 - accuracy: 0.9588 - val_loss: 0.8533 - val_accuracy: 0.7877 - lr: 1.7028e-05\n",
            "Epoch 80/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0953 - accuracy: 0.9700\n",
            "Epoch 80: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0892 - accuracy: 0.9726 - val_loss: 0.8353 - val_accuracy: 0.8014 - lr: 1.7028e-05\n",
            "Epoch 81/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0909 - accuracy: 0.9620\n",
            "Epoch 81: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0862 - accuracy: 0.9640 - val_loss: 0.7053 - val_accuracy: 0.7671 - lr: 1.6177e-05\n",
            "Epoch 82/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0978 - accuracy: 0.9660\n",
            "Epoch 82: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0918 - accuracy: 0.9657 - val_loss: 0.7527 - val_accuracy: 0.8014 - lr: 1.6177e-05\n",
            "Epoch 83/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0960 - accuracy: 0.9540\n",
            "Epoch 83: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0936 - accuracy: 0.9571 - val_loss: 0.7157 - val_accuracy: 0.8356 - lr: 1.6177e-05\n",
            "Epoch 84/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0658 - accuracy: 0.9780\n",
            "Epoch 84: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0634 - accuracy: 0.9777 - val_loss: 0.8474 - val_accuracy: 0.7877 - lr: 1.5368e-05\n",
            "Epoch 85/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0834 - accuracy: 0.9600\n",
            "Epoch 85: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0811 - accuracy: 0.9640 - val_loss: 0.7122 - val_accuracy: 0.8219 - lr: 1.5368e-05\n",
            "Epoch 86/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0738 - accuracy: 0.9740\n",
            "Epoch 86: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0708 - accuracy: 0.9760 - val_loss: 0.7853 - val_accuracy: 0.7945 - lr: 1.5368e-05\n",
            "Epoch 87/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.0921 - accuracy: 0.9644\n",
            "Epoch 87: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0846 - accuracy: 0.9657 - val_loss: 0.8896 - val_accuracy: 0.7877 - lr: 1.4599e-05\n",
            "Epoch 88/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0807 - accuracy: 0.9740\n",
            "Epoch 88: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0795 - accuracy: 0.9743 - val_loss: 0.7106 - val_accuracy: 0.8082 - lr: 1.4599e-05\n",
            "Epoch 89/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0819 - accuracy: 0.9720\n",
            "Epoch 89: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0836 - accuracy: 0.9708 - val_loss: 0.8036 - val_accuracy: 0.7945 - lr: 1.4599e-05\n",
            "Epoch 90/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0806 - accuracy: 0.9740\n",
            "Epoch 90: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0759 - accuracy: 0.9760 - val_loss: 0.6768 - val_accuracy: 0.7877 - lr: 1.3869e-05\n",
            "Epoch 91/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.0792 - accuracy: 0.9667\n",
            "Epoch 91: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0725 - accuracy: 0.9708 - val_loss: 0.8169 - val_accuracy: 0.7808 - lr: 1.3869e-05\n",
            "Epoch 92/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0611 - accuracy: 0.9840\n",
            "Epoch 92: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0568 - accuracy: 0.9863 - val_loss: 0.8521 - val_accuracy: 0.7877 - lr: 1.3869e-05\n",
            "Epoch 93/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.0711 - accuracy: 0.9756\n",
            "Epoch 93: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0680 - accuracy: 0.9777 - val_loss: 0.8154 - val_accuracy: 0.7945 - lr: 1.3176e-05\n",
            "Epoch 94/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0615 - accuracy: 0.9780\n",
            "Epoch 94: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0610 - accuracy: 0.9777 - val_loss: 0.7157 - val_accuracy: 0.8288 - lr: 1.3176e-05\n",
            "Epoch 95/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0711 - accuracy: 0.9760\n",
            "Epoch 95: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0685 - accuracy: 0.9760 - val_loss: 0.6769 - val_accuracy: 0.8151 - lr: 1.3176e-05\n",
            "Epoch 96/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0737 - accuracy: 0.9700\n",
            "Epoch 96: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0743 - accuracy: 0.9726 - val_loss: 0.9607 - val_accuracy: 0.7877 - lr: 1.2517e-05\n",
            "Epoch 97/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0729 - accuracy: 0.9720\n",
            "Epoch 97: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0709 - accuracy: 0.9726 - val_loss: 0.7250 - val_accuracy: 0.8288 - lr: 1.2517e-05\n",
            "Epoch 98/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0901 - accuracy: 0.9680\n",
            "Epoch 98: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0831 - accuracy: 0.9708 - val_loss: 0.6879 - val_accuracy: 0.8014 - lr: 1.2517e-05\n",
            "Epoch 99/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0712 - accuracy: 0.9800\n",
            "Epoch 99: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0728 - accuracy: 0.9811 - val_loss: 0.6760 - val_accuracy: 0.8219 - lr: 1.1891e-05\n",
            "Epoch 100/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.0748 - accuracy: 0.9720\n",
            "Epoch 100: val_loss did not improve from 0.52134\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0746 - accuracy: 0.9726 - val_loss: 0.7970 - val_accuracy: 0.8151 - lr: 1.1891e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fafd2609510>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "model.fit(dataset_train, epochs=100, callbacks=[reduce_lr, cp_callback], validation_data=dataset_valid)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "tOeSoxd0Dc3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "626f872a-5fd0-49a4-9317-1cd76876f396"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test model"
      ],
      "metadata": {
        "id": "JnC3v4hG4iEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = tf.keras.models.load_model('mirabest/cp.ckpt')"
      ],
      "metadata": {
        "id": "IxWv_8E_4hq2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = []\n",
        "accuracies = []\n",
        "for _ in range(samples_iter):\n",
        "  correct, n = 0, 0\n",
        "  s = []\n",
        "  for (x, y) in dataset_test:\n",
        "    yhat = new_model.predict(x)\n",
        "    correct += np.sum(np.argmax(yhat, axis=-1) == y)\n",
        "    n += len(y)\n",
        "    s.append(yhat)\n",
        "  s = np.concatenate(s)\n",
        "  softmax.append(s)\n",
        "  accuracies.append(correct / n)\n",
        "  print('Accuracy:',correct / n)\n",
        "softmax = np.array(softmax)"
      ],
      "metadata": {
        "id": "4z8JNVya_IzG",
        "outputId": "0ccbccf2-34a9-4dbd-e49e-fbebf8ffa385",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.7788461538461539\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.7692307692307693\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.7788461538461539\n",
            "Accuracy: 0.8269230769230769\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.8269230769230769\n",
            "Accuracy: 0.8269230769230769\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.7788461538461539\n",
            "Accuracy: 0.7788461538461539\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.7884615384615384\n",
            "Accuracy: 0.7692307692307693\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.8461538461538461\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.75\n",
            "Accuracy: 0.7884615384615384\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.7596153846153846\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.7692307692307693\n",
            "Accuracy: 0.7403846153846154\n",
            "Accuracy: 0.75\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.7884615384615384\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.7884615384615384\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.8269230769230769\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.7788461538461539\n",
            "Accuracy: 0.7596153846153846\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.7884615384615384\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.7692307692307693\n",
            "Accuracy: 0.8557692307692307\n",
            "Accuracy: 0.7692307692307693\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.7884615384615384\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.7788461538461539\n",
            "Accuracy: 0.7692307692307693\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.7788461538461539\n",
            "Accuracy: 0.8269230769230769\n",
            "Accuracy: 0.8269230769230769\n",
            "Accuracy: 0.7692307692307693\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.7788461538461539\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.7403846153846154\n",
            "Accuracy: 0.8269230769230769\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.7692307692307693\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.8269230769230769\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.8557692307692307\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.8461538461538461\n",
            "Accuracy: 0.8269230769230769\n",
            "Accuracy: 0.7692307692307693\n",
            "Accuracy: 0.8461538461538461\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.8557692307692307\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.7884615384615384\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.7884615384615384\n",
            "Accuracy: 0.7884615384615384\n",
            "Accuracy: 0.75\n",
            "Accuracy: 0.7788461538461539\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.8269230769230769\n",
            "Accuracy: 0.7788461538461539\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.8557692307692307\n",
            "Accuracy: 0.8269230769230769\n",
            "Accuracy: 0.8461538461538461\n",
            "Accuracy: 0.7884615384615384\n",
            "Accuracy: 0.8269230769230769\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.8461538461538461\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.8653846153846154\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.7596153846153846\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.7403846153846154\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.7884615384615384\n",
            "Accuracy: 0.8269230769230769\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.8461538461538461\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.7692307692307693\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.8269230769230769\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.7884615384615384\n",
            "Accuracy: 0.8269230769230769\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.7692307692307693\n",
            "Accuracy: 0.75\n",
            "Accuracy: 0.7596153846153846\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.7596153846153846\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.7596153846153846\n",
            "Accuracy: 0.8269230769230769\n",
            "Accuracy: 0.7884615384615384\n",
            "Accuracy: 0.8269230769230769\n",
            "Accuracy: 0.8269230769230769\n",
            "Accuracy: 0.8461538461538461\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.7788461538461539\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.7788461538461539\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.7692307692307693\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.8461538461538461\n",
            "Accuracy: 0.8269230769230769\n",
            "Accuracy: 0.8365384615384616\n",
            "Accuracy: 0.7596153846153846\n",
            "Accuracy: 0.7884615384615384\n",
            "Accuracy: 0.75\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.8076923076923077\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.8269230769230769\n",
            "Accuracy: 0.7884615384615384\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.8173076923076923\n",
            "Accuracy: 0.75\n",
            "Accuracy: 0.7980769230769231\n",
            "Accuracy: 0.7692307692307693\n",
            "Accuracy: 0.7596153846153846\n",
            "Accuracy: 0.8557692307692307\n",
            "Accuracy: 0.8269230769230769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy', np.mean(accuracies), np.std(accuracies))"
      ],
      "metadata": {
        "id": "02AZa6Np_6MX",
        "outputId": "c8dcc1fb-8c60-4343-e162-02ab7805b277",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.8060576923076923 0.027096112900553214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def uncertainty(softmax):\n",
        "  # Per example softmax with shape(num_examples, num_classes)\n",
        "  predictive_entropy = 0\n",
        "  single_pass_entropy = 0\n",
        "  for i in range(softmax.shape[1]):\n",
        "    # Sum over classes\n",
        "    predictive_entropy += -np.mean(softmax[:,i])*np.log(np.mean(softmax[:,i]))\n",
        "    single_pass_entropy += -softmax[:,i]*np.log(softmax[:,i])\n",
        "  single_pass_entropy = np.mean(single_pass_entropy)\n",
        "  mutual_info = predictive_entropy - single_pass_entropy\n",
        "  return predictive_entropy, single_pass_entropy, mutual_info"
      ],
      "metadata": {
        "id": "AFxGXi4JDnS3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictive_entropies = [] \n",
        "single_pass_entropies = []\n",
        "mutual_infos = []\n",
        "for i in range(softmax.shape[1]):\n",
        "  predictive_entropy, single_pass_entropy, mutual_info = uncertainty(softmax[:,i,:])\n",
        "  predictive_entropies.append(predictive_entropy)\n",
        "  single_pass_entropies.append(single_pass_entropy)\n",
        "  mutual_infos.append(mutual_info)\n",
        "print('Predictive entropy:', np.mean(predictive_entropies), np.std(predictive_entropies))\n",
        "print('Single pass entropy:', np.mean(single_pass_entropies), np.std(single_pass_entropies))\n",
        "print('Mutual info:', np.mean(mutual_infos), np.std(mutual_infos))"
      ],
      "metadata": {
        "id": "xGzYD6acDnVi",
        "outputId": "a21515d7-31fc-48d1-de2c-597e04f7de52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictive entropy: 0.36058097962182517 0.2216731205550558\n",
            "Single pass entropy: 0.27554953 0.1792092\n",
            "Mutual info: 0.08503145795921577 0.0613606875012375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(predictive_entropies)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SBnp5eiNDnYR",
        "outputId": "bc77bdfc-8d9b-4fba-af45-3f6e725a4352",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(single_pass_entropies)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8bPVRpEyDnan",
        "outputId": "28d9f486-b6b0-458b-87a0-e9cc4faf8ec4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(mutual_infos)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4IYJt9ocDndX",
        "outputId": "57977644-310c-45e7-985b-0a8c05014f63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "Mirabest_MCD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}