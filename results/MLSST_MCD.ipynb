{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adammoss/bnn_hmc/blob/main/results/MLSST_MCD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow_datasets as tfds\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "9kZAoIFwA7zk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples_iter = 200\n",
        "dropout = 0.1"
      ],
      "metadata": {
        "id": "pWYx2oo1A_U8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQUjf9c8hBnz",
        "outputId": "55f06909-e838-49ec-fb94-fef491a039ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: astro-datasets in /usr/local/lib/python3.7/dist-packages (0.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from astro-datasets) (1.21.6)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from astro-datasets) (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from astro-datasets) (4.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from astro-datasets) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->astro-datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->astro-datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->astro-datasets) (1.15.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (14.0.6)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (4.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (2.8.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (0.5.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (0.26.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (3.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (1.47.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (0.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (3.17.3)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (2.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (1.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->astro-datasets) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->astro-datasets) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (2.23.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (3.2.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->astro-datasets) (1.9.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->astro-datasets) (2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->astro-datasets) (4.64.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->astro-datasets) (0.6.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->astro-datasets) (5.9.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->astro-datasets) (0.10.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->astro-datasets) (0.3.5.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->astro-datasets) (1.56.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.3.5.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (5.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.21.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.2.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (3.17.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.1.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (4.1.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.23.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.9.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (4.64.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.24.3)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->tensorflow_datasets) (3.8.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.56.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install astro-datasets --upgrade\n",
        "!pip install tensorflow_datasets --upgrade\n",
        "import astro_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zXiWbueekbBn"
      },
      "outputs": [],
      "source": [
        "ds_train_Y10, info_train_Y10 = tfds.load(name='mlsst/Y10', split='train', with_info=True, as_supervised=True, batch_size=50)\n",
        "ds_valid_Y10, info_valid_Y10 = tfds.load(name='mlsst/Y10', split='validation', with_info=True, as_supervised=True, batch_size=50)\n",
        "ds_test_Y10, info_test_Y10 = tfds.load(name='mlsst/Y10', split='test', with_info=True, as_supervised=True, batch_size=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZXweQqAM653N"
      },
      "outputs": [],
      "source": [
        "def normalize(image, label):  \n",
        "  image = tf.math.asinh(image)\n",
        "  return image, label\n",
        "\n",
        "ds_train_Y10 = ds_train_Y10.map(normalize)\n",
        "ds_valid_Y10 = ds_valid_Y10.map(normalize)\n",
        "ds_test_Y10 = ds_test_Y10.map(normalize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ehoiezUclVoQ"
      },
      "outputs": [],
      "source": [
        "class MonteCarloDropout(tf.keras.layers.Dropout):\n",
        "     def call(self, inputs):\n",
        "         return super().call(inputs, training=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(6, kernel_size=5, strides=1, padding = 'same', input_shape=(100, 100, 3)),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2,2)),\n",
        "    MonteCarloDropout(dropout),\n",
        "    tf.keras.layers.Conv2D(16, kernel_size=5, strides=1, padding = 'same'),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2,2)),\n",
        "    MonteCarloDropout(dropout),\n",
        "    tf.keras.layers.Conv2D(120, kernel_size=5, strides=1, padding = 'same'),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2,2)),\n",
        "    MonteCarloDropout(dropout),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(84),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    MonteCarloDropout(dropout),\n",
        "    tf.keras.layers.Dense(3, activation='softmax'),\n",
        "    ])"
      ],
      "metadata": {
        "id": "nnXnsPC0A3XH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CQJOQvg8lZrJ",
        "outputId": "5038106e-f8df-4435-cb19-7bab6f999cf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 100, 100, 6)       456       \n",
            "                                                                 \n",
            " re_lu (ReLU)                (None, 100, 100, 6)       0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 49, 49, 6)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " monte_carlo_dropout (MonteC  (None, 49, 49, 6)        0         \n",
            " arloDropout)                                                    \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 49, 49, 16)        2416      \n",
            "                                                                 \n",
            " re_lu_1 (ReLU)              (None, 49, 49, 16)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 24, 24, 16)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " monte_carlo_dropout_1 (Mont  (None, 24, 24, 16)       0         \n",
            " eCarloDropout)                                                  \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 24, 24, 120)       48120     \n",
            "                                                                 \n",
            " re_lu_2 (ReLU)              (None, 24, 24, 120)       0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 11, 11, 120)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " monte_carlo_dropout_2 (Mont  (None, 11, 11, 120)      0         \n",
            " eCarloDropout)                                                  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 14520)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 84)                1219764   \n",
            "                                                                 \n",
            " re_lu_3 (ReLU)              (None, 84)                0         \n",
            "                                                                 \n",
            " monte_carlo_dropout_3 (Mont  (None, 84)               0         \n",
            " eCarloDropout)                                                  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 255       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,271,011\n",
            "Trainable params: 1,271,011\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "i27LHBdtmCdG"
      },
      "outputs": [],
      "source": [
        "loss=tf.keras.losses.SparseCategoricalCrossentropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UpC0XprymEwu"
      },
      "outputs": [],
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jHixKz5Rm12O"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=opt,\n",
        "              loss=loss,\n",
        "              metrics='accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kynByGwLm8w3"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"mlsst/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                  save_weights_only=False,\n",
        "                                                  monitor='val_loss',\n",
        "                                                  mode='min',\n",
        "                                                  verbose=1,\n",
        "                                                  save_best_only=True)\n",
        "\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2NIKK_tnBsx",
        "outputId": "6cbdf6e5-30e6-48f6-89a9-0ac6b027516e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "470/470 [==============================] - ETA: 0s - loss: 1.1197 - accuracy: 0.4776\n",
            "Epoch 1: val_loss improved from inf to 0.96703, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 12s 18ms/step - loss: 1.1197 - accuracy: 0.4776 - val_loss: 0.9670 - val_accuracy: 0.5159 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "465/470 [============================>.] - ETA: 0s - loss: 0.9499 - accuracy: 0.5390\n",
            "Epoch 2: val_loss improved from 0.96703 to 0.93351, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 16ms/step - loss: 0.9502 - accuracy: 0.5387 - val_loss: 0.9335 - val_accuracy: 0.5618 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "468/470 [============================>.] - ETA: 0s - loss: 0.9344 - accuracy: 0.5497\n",
            "Epoch 3: val_loss improved from 0.93351 to 0.91620, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 16ms/step - loss: 0.9350 - accuracy: 0.5498 - val_loss: 0.9162 - val_accuracy: 0.5690 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "466/470 [============================>.] - ETA: 0s - loss: 0.9267 - accuracy: 0.5560\n",
            "Epoch 4: val_loss did not improve from 0.91620\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.9274 - accuracy: 0.5559 - val_loss: 0.9248 - val_accuracy: 0.5523 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "467/470 [============================>.] - ETA: 0s - loss: 0.9221 - accuracy: 0.5568\n",
            "Epoch 5: val_loss improved from 0.91620 to 0.91301, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 7s 16ms/step - loss: 0.9226 - accuracy: 0.5569 - val_loss: 0.9130 - val_accuracy: 0.5690 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.9192 - accuracy: 0.5608\n",
            "Epoch 6: val_loss improved from 0.91301 to 0.90969, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 16ms/step - loss: 0.9195 - accuracy: 0.5605 - val_loss: 0.9097 - val_accuracy: 0.5717 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.9171 - accuracy: 0.5641\n",
            "Epoch 7: val_loss did not improve from 0.90969\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.9175 - accuracy: 0.5639 - val_loss: 0.9103 - val_accuracy: 0.5672 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "465/470 [============================>.] - ETA: 0s - loss: 0.9113 - accuracy: 0.5661\n",
            "Epoch 8: val_loss did not improve from 0.90969\n",
            "470/470 [==============================] - 6s 14ms/step - loss: 0.9116 - accuracy: 0.5662 - val_loss: 0.9119 - val_accuracy: 0.5633 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "470/470 [==============================] - ETA: 0s - loss: 0.9105 - accuracy: 0.5692\n",
            "Epoch 9: val_loss improved from 0.90969 to 0.90343, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 16ms/step - loss: 0.9105 - accuracy: 0.5692 - val_loss: 0.9034 - val_accuracy: 0.5759 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "468/470 [============================>.] - ETA: 0s - loss: 0.9037 - accuracy: 0.5754\n",
            "Epoch 10: val_loss improved from 0.90343 to 0.89852, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 16ms/step - loss: 0.9042 - accuracy: 0.5754 - val_loss: 0.8985 - val_accuracy: 0.5821 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "465/470 [============================>.] - ETA: 0s - loss: 0.8965 - accuracy: 0.5798\n",
            "Epoch 11: val_loss improved from 0.89852 to 0.89631, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 7s 16ms/step - loss: 0.8971 - accuracy: 0.5796 - val_loss: 0.8963 - val_accuracy: 0.5765 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.8908 - accuracy: 0.5829\n",
            "Epoch 12: val_loss improved from 0.89631 to 0.89039, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 16ms/step - loss: 0.8911 - accuracy: 0.5827 - val_loss: 0.8904 - val_accuracy: 0.5887 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "466/470 [============================>.] - ETA: 0s - loss: 0.8858 - accuracy: 0.5877\n",
            "Epoch 13: val_loss improved from 0.89039 to 0.88236, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 17ms/step - loss: 0.8861 - accuracy: 0.5876 - val_loss: 0.8824 - val_accuracy: 0.5985 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "470/470 [==============================] - ETA: 0s - loss: 0.8774 - accuracy: 0.5917\n",
            "Epoch 14: val_loss improved from 0.88236 to 0.88124, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 16ms/step - loss: 0.8774 - accuracy: 0.5917 - val_loss: 0.8812 - val_accuracy: 0.6003 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.8709 - accuracy: 0.5957\n",
            "Epoch 15: val_loss improved from 0.88124 to 0.86959, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 16ms/step - loss: 0.8713 - accuracy: 0.5954 - val_loss: 0.8696 - val_accuracy: 0.6072 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "468/470 [============================>.] - ETA: 0s - loss: 0.8647 - accuracy: 0.5959\n",
            "Epoch 16: val_loss improved from 0.86959 to 0.85902, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 16ms/step - loss: 0.8651 - accuracy: 0.5958 - val_loss: 0.8590 - val_accuracy: 0.6057 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "470/470 [==============================] - ETA: 0s - loss: 0.8632 - accuracy: 0.5984\n",
            "Epoch 17: val_loss improved from 0.85902 to 0.85813, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 16ms/step - loss: 0.8632 - accuracy: 0.5984 - val_loss: 0.8581 - val_accuracy: 0.5994 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.8566 - accuracy: 0.5995\n",
            "Epoch 18: val_loss did not improve from 0.85813\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.8570 - accuracy: 0.5994 - val_loss: 0.8648 - val_accuracy: 0.5985 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "466/470 [============================>.] - ETA: 0s - loss: 0.8573 - accuracy: 0.5982\n",
            "Epoch 19: val_loss did not improve from 0.85813\n",
            "470/470 [==============================] - 6s 14ms/step - loss: 0.8577 - accuracy: 0.5980 - val_loss: 0.8598 - val_accuracy: 0.6030 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "466/470 [============================>.] - ETA: 0s - loss: 0.8536 - accuracy: 0.5992\n",
            "Epoch 20: val_loss improved from 0.85813 to 0.85668, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 16ms/step - loss: 0.8540 - accuracy: 0.5992 - val_loss: 0.8567 - val_accuracy: 0.6066 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "468/470 [============================>.] - ETA: 0s - loss: 0.8493 - accuracy: 0.6018\n",
            "Epoch 21: val_loss did not improve from 0.85668\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.8496 - accuracy: 0.6017 - val_loss: 0.8592 - val_accuracy: 0.6036 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "466/470 [============================>.] - ETA: 0s - loss: 0.8513 - accuracy: 0.6031\n",
            "Epoch 22: val_loss did not improve from 0.85668\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.8516 - accuracy: 0.6029 - val_loss: 0.8578 - val_accuracy: 0.6060 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "466/470 [============================>.] - ETA: 0s - loss: 0.8471 - accuracy: 0.6060\n",
            "Epoch 23: val_loss did not improve from 0.85668\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.8474 - accuracy: 0.6059 - val_loss: 0.8622 - val_accuracy: 0.6018 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "468/470 [============================>.] - ETA: 0s - loss: 0.8425 - accuracy: 0.6056\n",
            "Epoch 24: val_loss did not improve from 0.85668\n",
            "470/470 [==============================] - 7s 15ms/step - loss: 0.8429 - accuracy: 0.6055 - val_loss: 0.8609 - val_accuracy: 0.6012 - lr: 4.7500e-05\n",
            "Epoch 25/100\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.8400 - accuracy: 0.6076\n",
            "Epoch 25: val_loss improved from 0.85668 to 0.85615, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 16ms/step - loss: 0.8404 - accuracy: 0.6074 - val_loss: 0.8562 - val_accuracy: 0.6042 - lr: 4.7500e-05\n",
            "Epoch 26/100\n",
            "466/470 [============================>.] - ETA: 0s - loss: 0.8372 - accuracy: 0.6079\n",
            "Epoch 26: val_loss did not improve from 0.85615\n",
            "470/470 [==============================] - 6s 14ms/step - loss: 0.8377 - accuracy: 0.6078 - val_loss: 0.8625 - val_accuracy: 0.5937 - lr: 4.7500e-05\n",
            "Epoch 27/100\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.8352 - accuracy: 0.6092\n",
            "Epoch 27: val_loss did not improve from 0.85615\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.8355 - accuracy: 0.6091 - val_loss: 0.8568 - val_accuracy: 0.6036 - lr: 4.7500e-05\n",
            "Epoch 28/100\n",
            "468/470 [============================>.] - ETA: 0s - loss: 0.8328 - accuracy: 0.6125\n",
            "Epoch 28: val_loss improved from 0.85615 to 0.84942, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 17ms/step - loss: 0.8332 - accuracy: 0.6125 - val_loss: 0.8494 - val_accuracy: 0.6116 - lr: 4.7500e-05\n",
            "Epoch 29/100\n",
            "465/470 [============================>.] - ETA: 0s - loss: 0.8288 - accuracy: 0.6136\n",
            "Epoch 29: val_loss did not improve from 0.84942\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.8292 - accuracy: 0.6136 - val_loss: 0.8519 - val_accuracy: 0.6125 - lr: 4.7500e-05\n",
            "Epoch 30/100\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.8256 - accuracy: 0.6149\n",
            "Epoch 30: val_loss improved from 0.84942 to 0.84411, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 16ms/step - loss: 0.8259 - accuracy: 0.6148 - val_loss: 0.8441 - val_accuracy: 0.6140 - lr: 4.7500e-05\n",
            "Epoch 31/100\n",
            "466/470 [============================>.] - ETA: 0s - loss: 0.8209 - accuracy: 0.6202\n",
            "Epoch 31: val_loss did not improve from 0.84411\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.8210 - accuracy: 0.6204 - val_loss: 0.8515 - val_accuracy: 0.6072 - lr: 4.7500e-05\n",
            "Epoch 32/100\n",
            "470/470 [==============================] - ETA: 0s - loss: 0.8175 - accuracy: 0.6194\n",
            "Epoch 32: val_loss did not improve from 0.84411\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.8175 - accuracy: 0.6194 - val_loss: 0.8486 - val_accuracy: 0.6140 - lr: 4.7500e-05\n",
            "Epoch 33/100\n",
            "465/470 [============================>.] - ETA: 0s - loss: 0.8140 - accuracy: 0.6235\n",
            "Epoch 33: val_loss improved from 0.84411 to 0.84330, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 16ms/step - loss: 0.8140 - accuracy: 0.6235 - val_loss: 0.8433 - val_accuracy: 0.6164 - lr: 4.7500e-05\n",
            "Epoch 34/100\n",
            "465/470 [============================>.] - ETA: 0s - loss: 0.8128 - accuracy: 0.6217\n",
            "Epoch 34: val_loss did not improve from 0.84330\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.8131 - accuracy: 0.6215 - val_loss: 0.8496 - val_accuracy: 0.6191 - lr: 4.7500e-05\n",
            "Epoch 35/100\n",
            "466/470 [============================>.] - ETA: 0s - loss: 0.8063 - accuracy: 0.6257\n",
            "Epoch 35: val_loss did not improve from 0.84330\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.8066 - accuracy: 0.6255 - val_loss: 0.8486 - val_accuracy: 0.6137 - lr: 4.7500e-05\n",
            "Epoch 36/100\n",
            "470/470 [==============================] - ETA: 0s - loss: 0.7992 - accuracy: 0.6288\n",
            "Epoch 36: val_loss did not improve from 0.84330\n",
            "470/470 [==============================] - 6s 14ms/step - loss: 0.7992 - accuracy: 0.6288 - val_loss: 0.8452 - val_accuracy: 0.6176 - lr: 4.7500e-05\n",
            "Epoch 37/100\n",
            "468/470 [============================>.] - ETA: 0s - loss: 0.7979 - accuracy: 0.6295\n",
            "Epoch 37: val_loss improved from 0.84330 to 0.83034, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 17ms/step - loss: 0.7980 - accuracy: 0.6294 - val_loss: 0.8303 - val_accuracy: 0.6188 - lr: 4.5125e-05\n",
            "Epoch 38/100\n",
            "467/470 [============================>.] - ETA: 0s - loss: 0.7946 - accuracy: 0.6301\n",
            "Epoch 38: val_loss did not improve from 0.83034\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.7946 - accuracy: 0.6301 - val_loss: 0.8410 - val_accuracy: 0.6268 - lr: 4.5125e-05\n",
            "Epoch 39/100\n",
            "464/470 [============================>.] - ETA: 0s - loss: 0.7929 - accuracy: 0.6329\n",
            "Epoch 39: val_loss did not improve from 0.83034\n",
            "470/470 [==============================] - 6s 14ms/step - loss: 0.7929 - accuracy: 0.6329 - val_loss: 0.8386 - val_accuracy: 0.6191 - lr: 4.5125e-05\n",
            "Epoch 40/100\n",
            "466/470 [============================>.] - ETA: 0s - loss: 0.7839 - accuracy: 0.6334\n",
            "Epoch 40: val_loss did not improve from 0.83034\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.7843 - accuracy: 0.6332 - val_loss: 0.8355 - val_accuracy: 0.6158 - lr: 4.5125e-05\n",
            "Epoch 41/100\n",
            "464/470 [============================>.] - ETA: 0s - loss: 0.7814 - accuracy: 0.6354\n",
            "Epoch 41: val_loss did not improve from 0.83034\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.7815 - accuracy: 0.6354 - val_loss: 0.8411 - val_accuracy: 0.6164 - lr: 4.2869e-05\n",
            "Epoch 42/100\n",
            "468/470 [============================>.] - ETA: 0s - loss: 0.7744 - accuracy: 0.6410\n",
            "Epoch 42: val_loss did not improve from 0.83034\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.7746 - accuracy: 0.6409 - val_loss: 0.8366 - val_accuracy: 0.6191 - lr: 4.2869e-05\n",
            "Epoch 43/100\n",
            "470/470 [==============================] - ETA: 0s - loss: 0.7720 - accuracy: 0.6405\n",
            "Epoch 43: val_loss did not improve from 0.83034\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.7720 - accuracy: 0.6405 - val_loss: 0.8356 - val_accuracy: 0.6274 - lr: 4.2869e-05\n",
            "Epoch 44/100\n",
            "468/470 [============================>.] - ETA: 0s - loss: 0.7699 - accuracy: 0.6436\n",
            "Epoch 44: val_loss improved from 0.83034 to 0.82908, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 16ms/step - loss: 0.7700 - accuracy: 0.6436 - val_loss: 0.8291 - val_accuracy: 0.6227 - lr: 4.0725e-05\n",
            "Epoch 45/100\n",
            "464/470 [============================>.] - ETA: 0s - loss: 0.7665 - accuracy: 0.6427\n",
            "Epoch 45: val_loss did not improve from 0.82908\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.7664 - accuracy: 0.6424 - val_loss: 0.8432 - val_accuracy: 0.6238 - lr: 4.0725e-05\n",
            "Epoch 46/100\n",
            "466/470 [============================>.] - ETA: 0s - loss: 0.7634 - accuracy: 0.6470\n",
            "Epoch 46: val_loss did not improve from 0.82908\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.7635 - accuracy: 0.6471 - val_loss: 0.8352 - val_accuracy: 0.6221 - lr: 4.0725e-05\n",
            "Epoch 47/100\n",
            "467/470 [============================>.] - ETA: 0s - loss: 0.7603 - accuracy: 0.6448\n",
            "Epoch 47: val_loss did not improve from 0.82908\n",
            "470/470 [==============================] - 6s 14ms/step - loss: 0.7604 - accuracy: 0.6448 - val_loss: 0.8320 - val_accuracy: 0.6218 - lr: 4.0725e-05\n",
            "Epoch 48/100\n",
            "468/470 [============================>.] - ETA: 0s - loss: 0.7526 - accuracy: 0.6507\n",
            "Epoch 48: val_loss improved from 0.82908 to 0.82772, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 16ms/step - loss: 0.7528 - accuracy: 0.6505 - val_loss: 0.8277 - val_accuracy: 0.6244 - lr: 3.8689e-05\n",
            "Epoch 49/100\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.7518 - accuracy: 0.6521\n",
            "Epoch 49: val_loss did not improve from 0.82772\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.7521 - accuracy: 0.6519 - val_loss: 0.8452 - val_accuracy: 0.6173 - lr: 3.8689e-05\n",
            "Epoch 50/100\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.7502 - accuracy: 0.6524\n",
            "Epoch 50: val_loss improved from 0.82772 to 0.82003, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 17ms/step - loss: 0.7504 - accuracy: 0.6522 - val_loss: 0.8200 - val_accuracy: 0.6268 - lr: 3.8689e-05\n",
            "Epoch 51/100\n",
            "468/470 [============================>.] - ETA: 0s - loss: 0.7373 - accuracy: 0.6563\n",
            "Epoch 51: val_loss did not improve from 0.82003\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.7372 - accuracy: 0.6564 - val_loss: 0.8393 - val_accuracy: 0.6232 - lr: 3.8689e-05\n",
            "Epoch 52/100\n",
            "467/470 [============================>.] - ETA: 0s - loss: 0.7332 - accuracy: 0.6612\n",
            "Epoch 52: val_loss did not improve from 0.82003\n",
            "470/470 [==============================] - 6s 14ms/step - loss: 0.7333 - accuracy: 0.6611 - val_loss: 0.8268 - val_accuracy: 0.6274 - lr: 3.8689e-05\n",
            "Epoch 53/100\n",
            "465/470 [============================>.] - ETA: 0s - loss: 0.7324 - accuracy: 0.6612\n",
            "Epoch 53: val_loss did not improve from 0.82003\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.7322 - accuracy: 0.6612 - val_loss: 0.8384 - val_accuracy: 0.6191 - lr: 3.8689e-05\n",
            "Epoch 54/100\n",
            "467/470 [============================>.] - ETA: 0s - loss: 0.7282 - accuracy: 0.6620\n",
            "Epoch 54: val_loss did not improve from 0.82003\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.7281 - accuracy: 0.6618 - val_loss: 0.8312 - val_accuracy: 0.6322 - lr: 3.6755e-05\n",
            "Epoch 55/100\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.7286 - accuracy: 0.6598\n",
            "Epoch 55: val_loss did not improve from 0.82003\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.7287 - accuracy: 0.6596 - val_loss: 0.8311 - val_accuracy: 0.6227 - lr: 3.6755e-05\n",
            "Epoch 56/100\n",
            "468/470 [============================>.] - ETA: 0s - loss: 0.7207 - accuracy: 0.6665\n",
            "Epoch 56: val_loss did not improve from 0.82003\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.7207 - accuracy: 0.6662 - val_loss: 0.8269 - val_accuracy: 0.6304 - lr: 3.6755e-05\n",
            "Epoch 57/100\n",
            "470/470 [==============================] - ETA: 0s - loss: 0.7178 - accuracy: 0.6635\n",
            "Epoch 57: val_loss improved from 0.82003 to 0.81653, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 16ms/step - loss: 0.7178 - accuracy: 0.6635 - val_loss: 0.8165 - val_accuracy: 0.6379 - lr: 3.4917e-05\n",
            "Epoch 58/100\n",
            "466/470 [============================>.] - ETA: 0s - loss: 0.7074 - accuracy: 0.6694\n",
            "Epoch 58: val_loss did not improve from 0.81653\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.7074 - accuracy: 0.6693 - val_loss: 0.8274 - val_accuracy: 0.6206 - lr: 3.4917e-05\n",
            "Epoch 59/100\n",
            "468/470 [============================>.] - ETA: 0s - loss: 0.7115 - accuracy: 0.6670\n",
            "Epoch 59: val_loss did not improve from 0.81653\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.7114 - accuracy: 0.6670 - val_loss: 0.8324 - val_accuracy: 0.6268 - lr: 3.4917e-05\n",
            "Epoch 60/100\n",
            "467/470 [============================>.] - ETA: 0s - loss: 0.7007 - accuracy: 0.6746\n",
            "Epoch 60: val_loss did not improve from 0.81653\n",
            "470/470 [==============================] - 6s 14ms/step - loss: 0.7006 - accuracy: 0.6747 - val_loss: 0.8172 - val_accuracy: 0.6280 - lr: 3.4917e-05\n",
            "Epoch 61/100\n",
            "467/470 [============================>.] - ETA: 0s - loss: 0.6993 - accuracy: 0.6741\n",
            "Epoch 61: val_loss did not improve from 0.81653\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.6995 - accuracy: 0.6737 - val_loss: 0.8208 - val_accuracy: 0.6355 - lr: 3.3171e-05\n",
            "Epoch 62/100\n",
            "466/470 [============================>.] - ETA: 0s - loss: 0.7045 - accuracy: 0.6728\n",
            "Epoch 62: val_loss did not improve from 0.81653\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.7045 - accuracy: 0.6727 - val_loss: 0.8211 - val_accuracy: 0.6298 - lr: 3.3171e-05\n",
            "Epoch 63/100\n",
            "467/470 [============================>.] - ETA: 0s - loss: 0.6926 - accuracy: 0.6794\n",
            "Epoch 63: val_loss improved from 0.81653 to 0.81316, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 16ms/step - loss: 0.6926 - accuracy: 0.6794 - val_loss: 0.8132 - val_accuracy: 0.6393 - lr: 3.3171e-05\n",
            "Epoch 64/100\n",
            "470/470 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.6767\n",
            "Epoch 64: val_loss did not improve from 0.81316\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.6943 - accuracy: 0.6767 - val_loss: 0.8213 - val_accuracy: 0.6337 - lr: 3.3171e-05\n",
            "Epoch 65/100\n",
            "466/470 [============================>.] - ETA: 0s - loss: 0.6860 - accuracy: 0.6797\n",
            "Epoch 65: val_loss improved from 0.81316 to 0.80964, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 17ms/step - loss: 0.6858 - accuracy: 0.6795 - val_loss: 0.8096 - val_accuracy: 0.6274 - lr: 3.3171e-05\n",
            "Epoch 66/100\n",
            "468/470 [============================>.] - ETA: 0s - loss: 0.6850 - accuracy: 0.6834\n",
            "Epoch 66: val_loss did not improve from 0.80964\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.6848 - accuracy: 0.6835 - val_loss: 0.8166 - val_accuracy: 0.6376 - lr: 3.3171e-05\n",
            "Epoch 67/100\n",
            "467/470 [============================>.] - ETA: 0s - loss: 0.6819 - accuracy: 0.6830\n",
            "Epoch 67: val_loss did not improve from 0.80964\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.6819 - accuracy: 0.6827 - val_loss: 0.8105 - val_accuracy: 0.6373 - lr: 3.3171e-05\n",
            "Epoch 68/100\n",
            "465/470 [============================>.] - ETA: 0s - loss: 0.6768 - accuracy: 0.6843\n",
            "Epoch 68: val_loss did not improve from 0.80964\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.6772 - accuracy: 0.6842 - val_loss: 0.8299 - val_accuracy: 0.6328 - lr: 3.3171e-05\n",
            "Epoch 69/100\n",
            "464/470 [============================>.] - ETA: 0s - loss: 0.6735 - accuracy: 0.6859\n",
            "Epoch 69: val_loss did not improve from 0.80964\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.6737 - accuracy: 0.6854 - val_loss: 0.8235 - val_accuracy: 0.6370 - lr: 3.1512e-05\n",
            "Epoch 70/100\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.6714 - accuracy: 0.6881\n",
            "Epoch 70: val_loss did not improve from 0.80964\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.6714 - accuracy: 0.6882 - val_loss: 0.8236 - val_accuracy: 0.6402 - lr: 3.1512e-05\n",
            "Epoch 71/100\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.6641 - accuracy: 0.6897\n",
            "Epoch 71: val_loss improved from 0.80964 to 0.80740, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 16ms/step - loss: 0.6640 - accuracy: 0.6897 - val_loss: 0.8074 - val_accuracy: 0.6423 - lr: 3.1512e-05\n",
            "Epoch 72/100\n",
            "467/470 [============================>.] - ETA: 0s - loss: 0.6599 - accuracy: 0.6958\n",
            "Epoch 72: val_loss did not improve from 0.80740\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.6599 - accuracy: 0.6957 - val_loss: 0.8166 - val_accuracy: 0.6373 - lr: 3.1512e-05\n",
            "Epoch 73/100\n",
            "465/470 [============================>.] - ETA: 0s - loss: 0.6562 - accuracy: 0.6932\n",
            "Epoch 73: val_loss did not improve from 0.80740\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.6559 - accuracy: 0.6929 - val_loss: 0.8135 - val_accuracy: 0.6370 - lr: 3.1512e-05\n",
            "Epoch 74/100\n",
            "467/470 [============================>.] - ETA: 0s - loss: 0.6515 - accuracy: 0.6956\n",
            "Epoch 74: val_loss did not improve from 0.80740\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.6513 - accuracy: 0.6953 - val_loss: 0.8445 - val_accuracy: 0.6396 - lr: 3.1512e-05\n",
            "Epoch 75/100\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.6457 - accuracy: 0.6995\n",
            "Epoch 75: val_loss did not improve from 0.80740\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.6456 - accuracy: 0.6994 - val_loss: 0.8186 - val_accuracy: 0.6379 - lr: 2.9937e-05\n",
            "Epoch 76/100\n",
            "464/470 [============================>.] - ETA: 0s - loss: 0.6505 - accuracy: 0.6975\n",
            "Epoch 76: val_loss did not improve from 0.80740\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.6508 - accuracy: 0.6976 - val_loss: 0.8192 - val_accuracy: 0.6465 - lr: 2.9937e-05\n",
            "Epoch 77/100\n",
            "465/470 [============================>.] - ETA: 0s - loss: 0.6432 - accuracy: 0.7019\n",
            "Epoch 77: val_loss did not improve from 0.80740\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.6436 - accuracy: 0.7020 - val_loss: 0.8210 - val_accuracy: 0.6387 - lr: 2.9937e-05\n",
            "Epoch 78/100\n",
            "465/470 [============================>.] - ETA: 0s - loss: 0.6430 - accuracy: 0.6987\n",
            "Epoch 78: val_loss did not improve from 0.80740\n",
            "470/470 [==============================] - 6s 14ms/step - loss: 0.6431 - accuracy: 0.6986 - val_loss: 0.8083 - val_accuracy: 0.6447 - lr: 2.8440e-05\n",
            "Epoch 79/100\n",
            "468/470 [============================>.] - ETA: 0s - loss: 0.6347 - accuracy: 0.7077\n",
            "Epoch 79: val_loss improved from 0.80740 to 0.80477, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 16ms/step - loss: 0.6346 - accuracy: 0.7076 - val_loss: 0.8048 - val_accuracy: 0.6447 - lr: 2.8440e-05\n",
            "Epoch 80/100\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.6336 - accuracy: 0.7058\n",
            "Epoch 80: val_loss did not improve from 0.80477\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.6337 - accuracy: 0.7058 - val_loss: 0.8268 - val_accuracy: 0.6438 - lr: 2.8440e-05\n",
            "Epoch 81/100\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.6322 - accuracy: 0.7029\n",
            "Epoch 81: val_loss did not improve from 0.80477\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.6322 - accuracy: 0.7029 - val_loss: 0.8205 - val_accuracy: 0.6423 - lr: 2.8440e-05\n",
            "Epoch 82/100\n",
            "466/470 [============================>.] - ETA: 0s - loss: 0.6225 - accuracy: 0.7089\n",
            "Epoch 82: val_loss did not improve from 0.80477\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.6228 - accuracy: 0.7086 - val_loss: 0.8211 - val_accuracy: 0.6417 - lr: 2.8440e-05\n",
            "Epoch 83/100\n",
            "470/470 [==============================] - ETA: 0s - loss: 0.6212 - accuracy: 0.7104\n",
            "Epoch 83: val_loss did not improve from 0.80477\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.6212 - accuracy: 0.7104 - val_loss: 0.8174 - val_accuracy: 0.6492 - lr: 2.7018e-05\n",
            "Epoch 84/100\n",
            "465/470 [============================>.] - ETA: 0s - loss: 0.6158 - accuracy: 0.7127\n",
            "Epoch 84: val_loss did not improve from 0.80477\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.6161 - accuracy: 0.7126 - val_loss: 0.8390 - val_accuracy: 0.6432 - lr: 2.7018e-05\n",
            "Epoch 85/100\n",
            "467/470 [============================>.] - ETA: 0s - loss: 0.6225 - accuracy: 0.7096\n",
            "Epoch 85: val_loss did not improve from 0.80477\n",
            "470/470 [==============================] - 6s 14ms/step - loss: 0.6223 - accuracy: 0.7096 - val_loss: 0.8113 - val_accuracy: 0.6539 - lr: 2.7018e-05\n",
            "Epoch 86/100\n",
            "465/470 [============================>.] - ETA: 0s - loss: 0.6188 - accuracy: 0.7109\n",
            "Epoch 86: val_loss did not improve from 0.80477\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.6187 - accuracy: 0.7108 - val_loss: 0.8131 - val_accuracy: 0.6435 - lr: 2.5667e-05\n",
            "Epoch 87/100\n",
            "470/470 [==============================] - ETA: 0s - loss: 0.6079 - accuracy: 0.7153\n",
            "Epoch 87: val_loss did not improve from 0.80477\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.6079 - accuracy: 0.7153 - val_loss: 0.8311 - val_accuracy: 0.6373 - lr: 2.5667e-05\n",
            "Epoch 88/100\n",
            "465/470 [============================>.] - ETA: 0s - loss: 0.6054 - accuracy: 0.7190\n",
            "Epoch 88: val_loss did not improve from 0.80477\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.6056 - accuracy: 0.7190 - val_loss: 0.8244 - val_accuracy: 0.6420 - lr: 2.5667e-05\n",
            "Epoch 89/100\n",
            "468/470 [============================>.] - ETA: 0s - loss: 0.6017 - accuracy: 0.7224\n",
            "Epoch 89: val_loss did not improve from 0.80477\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.6022 - accuracy: 0.7222 - val_loss: 0.8120 - val_accuracy: 0.6492 - lr: 2.4384e-05\n",
            "Epoch 90/100\n",
            "468/470 [============================>.] - ETA: 0s - loss: 0.6015 - accuracy: 0.7211\n",
            "Epoch 90: val_loss did not improve from 0.80477\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.6014 - accuracy: 0.7211 - val_loss: 0.8436 - val_accuracy: 0.6402 - lr: 2.4384e-05\n",
            "Epoch 91/100\n",
            "468/470 [============================>.] - ETA: 0s - loss: 0.6021 - accuracy: 0.7222\n",
            "Epoch 91: val_loss improved from 0.80477 to 0.80294, saving model to mlsst/cp.ckpt\n",
            "470/470 [==============================] - 8s 16ms/step - loss: 0.6020 - accuracy: 0.7224 - val_loss: 0.8029 - val_accuracy: 0.6644 - lr: 2.4384e-05\n",
            "Epoch 92/100\n",
            "467/470 [============================>.] - ETA: 0s - loss: 0.5952 - accuracy: 0.7242\n",
            "Epoch 92: val_loss did not improve from 0.80294\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.5947 - accuracy: 0.7245 - val_loss: 0.8148 - val_accuracy: 0.6441 - lr: 2.4384e-05\n",
            "Epoch 93/100\n",
            "468/470 [============================>.] - ETA: 0s - loss: 0.5903 - accuracy: 0.7277\n",
            "Epoch 93: val_loss did not improve from 0.80294\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.5901 - accuracy: 0.7278 - val_loss: 0.8245 - val_accuracy: 0.6402 - lr: 2.4384e-05\n",
            "Epoch 94/100\n",
            "466/470 [============================>.] - ETA: 0s - loss: 0.5909 - accuracy: 0.7291\n",
            "Epoch 94: val_loss did not improve from 0.80294\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.5910 - accuracy: 0.7290 - val_loss: 0.8136 - val_accuracy: 0.6420 - lr: 2.4384e-05\n",
            "Epoch 95/100\n",
            "468/470 [============================>.] - ETA: 0s - loss: 0.5849 - accuracy: 0.7310\n",
            "Epoch 95: val_loss did not improve from 0.80294\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.5850 - accuracy: 0.7309 - val_loss: 0.8194 - val_accuracy: 0.6441 - lr: 2.3165e-05\n",
            "Epoch 96/100\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.5854 - accuracy: 0.7313\n",
            "Epoch 96: val_loss did not improve from 0.80294\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.5854 - accuracy: 0.7313 - val_loss: 0.8156 - val_accuracy: 0.6623 - lr: 2.3165e-05\n",
            "Epoch 97/100\n",
            "465/470 [============================>.] - ETA: 0s - loss: 0.5842 - accuracy: 0.7326\n",
            "Epoch 97: val_loss did not improve from 0.80294\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.5841 - accuracy: 0.7328 - val_loss: 0.8322 - val_accuracy: 0.6495 - lr: 2.3165e-05\n",
            "Epoch 98/100\n",
            "469/470 [============================>.] - ETA: 0s - loss: 0.5774 - accuracy: 0.7395\n",
            "Epoch 98: val_loss did not improve from 0.80294\n",
            "470/470 [==============================] - 6s 13ms/step - loss: 0.5771 - accuracy: 0.7397 - val_loss: 0.8309 - val_accuracy: 0.6444 - lr: 2.2006e-05\n",
            "Epoch 99/100\n",
            "468/470 [============================>.] - ETA: 0s - loss: 0.5765 - accuracy: 0.7359\n",
            "Epoch 99: val_loss did not improve from 0.80294\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.5765 - accuracy: 0.7358 - val_loss: 0.8129 - val_accuracy: 0.6498 - lr: 2.2006e-05\n",
            "Epoch 100/100\n",
            "466/470 [============================>.] - ETA: 0s - loss: 0.5739 - accuracy: 0.7383\n",
            "Epoch 100: val_loss did not improve from 0.80294\n",
            "470/470 [==============================] - 7s 14ms/step - loss: 0.5739 - accuracy: 0.7383 - val_loss: 0.8362 - val_accuracy: 0.6408 - lr: 2.2006e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f191b1b0450>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "model.fit(ds_train_Y10, epochs=100, callbacks=[reduce_lr, cp_callback, es_callback], validation_data=ds_valid_Y10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RYDzwga4nTqE"
      },
      "outputs": [],
      "source": [
        "new_model = tf.keras.models.load_model('mlsst/cp.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = []\n",
        "accuracies = []\n",
        "for _ in range(samples_iter):\n",
        "  y = []\n",
        "  yhat = []\n",
        "  logits = []\n",
        "  for (x_batch, y_batch) in ds_test_Y10:\n",
        "      s = new_model.predict(x_batch)\n",
        "      yhat.append(np.argmax(s, axis=-1))\n",
        "      y.append(y_batch)\n",
        "      logits.append(s)\n",
        "  y = np.concatenate(y)\n",
        "  yhat = np.concatenate(yhat)\n",
        "  softmax.append(np.concatenate(logits))\n",
        "  accuracies.append(np.mean(yhat == y))\n",
        "  print('Accuracy:', np.mean(yhat == y))\n",
        "softmax = np.array(softmax)"
      ],
      "metadata": {
        "id": "VcEnYF6bzaoY",
        "outputId": "ac0fac50-20f0-4428-ce28-35a0e5ab19f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6302308265078184\n",
            "Accuracy: 0.6308265078183172\n",
            "Accuracy: 0.6297840655249441\n",
            "Accuracy: 0.6312732688011914\n",
            "Accuracy: 0.6290394638868205\n",
            "Accuracy: 0.6375279225614296\n",
            "Accuracy: 0.6275502606105734\n",
            "Accuracy: 0.6305286671630678\n",
            "Accuracy: 0.6269545793000745\n",
            "Accuracy: 0.6296351451973194\n",
            "Accuracy: 0.6303797468354431\n",
            "Accuracy: 0.6361876396128071\n",
            "Accuracy: 0.6324646314221891\n",
            "Accuracy: 0.6302308265078184\n",
            "Accuracy: 0.62933730454207\n",
            "Accuracy: 0.63469843633656\n",
            "Accuracy: 0.6287416232315711\n",
            "Accuracy: 0.6198064035740879\n",
            "Accuracy: 0.6326135517498138\n",
            "Accuracy: 0.637825763216679\n",
            "Accuracy: 0.6323157110945644\n",
            "Accuracy: 0.6248696947133283\n",
            "Accuracy: 0.6294862248696947\n",
            "Accuracy: 0.632017870439315\n",
            "Accuracy: 0.6291883842144452\n",
            "Accuracy: 0.6349962769918094\n",
            "Accuracy: 0.6315711094564408\n",
            "Accuracy: 0.6321667907669397\n",
            "Accuracy: 0.6279970215934475\n",
            "Accuracy: 0.6333581533879374\n",
            "Accuracy: 0.6305286671630678\n",
            "Accuracy: 0.6265078183172003\n",
            "Accuracy: 0.6288905435591958\n",
            "Accuracy: 0.6279970215934475\n",
            "Accuracy: 0.6215934475055845\n",
            "Accuracy: 0.6253164556962025\n",
            "Accuracy: 0.6282948622486969\n",
            "Accuracy: 0.6366344005956813\n",
            "Accuracy: 0.6275502606105734\n",
            "Accuracy: 0.6241250930752048\n",
            "Accuracy: 0.6303797468354431\n",
            "Accuracy: 0.6336559940431868\n",
            "Accuracy: 0.6244229337304542\n",
            "Accuracy: 0.6314221891288161\n",
            "Accuracy: 0.6366344005956813\n",
            "Accuracy: 0.6358897989575577\n",
            "Accuracy: 0.6390171258376769\n",
            "Accuracy: 0.6257632166790766\n",
            "Accuracy: 0.6230826507818317\n",
            "Accuracy: 0.6308265078183172\n",
            "Accuracy: 0.62933730454207\n",
            "Accuracy: 0.6360387192851824\n",
            "Accuracy: 0.6303797468354431\n",
            "Accuracy: 0.6303797468354431\n",
            "Accuracy: 0.6341027550260611\n",
            "Accuracy: 0.6275502606105734\n",
            "Accuracy: 0.6260610573343262\n",
            "Accuracy: 0.6349962769918094\n",
            "Accuracy: 0.6269545793000745\n",
            "Accuracy: 0.6279970215934475\n",
            "Accuracy: 0.6274013402829486\n",
            "Accuracy: 0.6297840655249441\n",
            "Accuracy: 0.638421444527178\n",
            "Accuracy: 0.6324646314221891\n",
            "Accuracy: 0.6312732688011914\n",
            "Accuracy: 0.6315711094564408\n",
            "Accuracy: 0.6336559940431868\n",
            "Accuracy: 0.6265078183172003\n",
            "Accuracy: 0.6291883842144452\n",
            "Accuracy: 0.6302308265078184\n",
            "Accuracy: 0.6302308265078184\n",
            "Accuracy: 0.6305286671630678\n",
            "Accuracy: 0.6285927029039464\n",
            "Accuracy: 0.6266567386448251\n",
            "Accuracy: 0.6318689501116902\n",
            "Accuracy: 0.6376768428890544\n",
            "Accuracy: 0.6265078183172003\n",
            "Accuracy: 0.6275502606105734\n",
            "Accuracy: 0.6363365599404319\n",
            "Accuracy: 0.6321667907669397\n",
            "Accuracy: 0.6312732688011914\n",
            "Accuracy: 0.6327624720774385\n",
            "Accuracy: 0.6306775874906925\n",
            "Accuracy: 0.6268056589724498\n",
            "Accuracy: 0.6321667907669397\n",
            "Accuracy: 0.6306775874906925\n",
            "Accuracy: 0.6300819061801936\n",
            "Accuracy: 0.6329113924050633\n",
            "Accuracy: 0.6296351451973194\n",
            "Accuracy: 0.6251675353685778\n",
            "Accuracy: 0.6274013402829486\n",
            "Accuracy: 0.6338049143708117\n",
            "Accuracy: 0.6358897989575577\n",
            "Accuracy: 0.6348473566641847\n",
            "Accuracy: 0.6302308265078184\n",
            "Accuracy: 0.6284437825763217\n",
            "Accuracy: 0.6296351451973194\n",
            "Accuracy: 0.6344005956813105\n",
            "Accuracy: 0.6345495160089352\n",
            "Accuracy: 0.6324646314221891\n",
            "Accuracy: 0.6345495160089352\n",
            "Accuracy: 0.6360387192851824\n",
            "Accuracy: 0.6259121370067015\n",
            "Accuracy: 0.6278481012658228\n",
            "Accuracy: 0.63469843633656\n",
            "Accuracy: 0.6259121370067015\n",
            "Accuracy: 0.62933730454207\n",
            "Accuracy: 0.6294862248696947\n",
            "Accuracy: 0.6315711094564408\n",
            "Accuracy: 0.636783320923306\n",
            "Accuracy: 0.6345495160089352\n",
            "Accuracy: 0.6235294117647059\n",
            "Accuracy: 0.6335070737155621\n",
            "Accuracy: 0.6300819061801936\n",
            "Accuracy: 0.6321667907669397\n",
            "Accuracy: 0.6291883842144452\n",
            "Accuracy: 0.6189128816083396\n",
            "Accuracy: 0.6244229337304542\n",
            "Accuracy: 0.6294862248696947\n",
            "Accuracy: 0.6317200297840655\n",
            "Accuracy: 0.637379002233805\n",
            "Accuracy: 0.6306775874906925\n",
            "Accuracy: 0.6349962769918094\n",
            "Accuracy: 0.633060312732688\n",
            "Accuracy: 0.6324646314221891\n",
            "Accuracy: 0.6351451973194341\n",
            "Accuracy: 0.6318689501116902\n",
            "Accuracy: 0.6242740134028295\n",
            "Accuracy: 0.633060312732688\n",
            "Accuracy: 0.6312732688011914\n",
            "Accuracy: 0.6303797468354431\n",
            "Accuracy: 0.6314221891288161\n",
            "Accuracy: 0.6329113924050633\n",
            "Accuracy: 0.6318689501116902\n",
            "Accuracy: 0.6282948622486969\n",
            "Accuracy: 0.6294862248696947\n",
            "Accuracy: 0.6278481012658228\n",
            "Accuracy: 0.6336559940431868\n",
            "Accuracy: 0.63469843633656\n",
            "Accuracy: 0.6260610573343262\n",
            "Accuracy: 0.6366344005956813\n",
            "Accuracy: 0.6305286671630678\n",
            "Accuracy: 0.6305286671630678\n",
            "Accuracy: 0.6314221891288161\n",
            "Accuracy: 0.6338049143708117\n",
            "Accuracy: 0.6303797468354431\n",
            "Accuracy: 0.6278481012658228\n",
            "Accuracy: 0.6259121370067015\n",
            "Accuracy: 0.6294862248696947\n",
            "Accuracy: 0.6341027550260611\n",
            "Accuracy: 0.6323157110945644\n",
            "Accuracy: 0.6376768428890544\n",
            "Accuracy: 0.6318689501116902\n",
            "Accuracy: 0.6288905435591958\n",
            "Accuracy: 0.6376768428890544\n",
            "Accuracy: 0.633060312732688\n",
            "Accuracy: 0.6314221891288161\n",
            "Accuracy: 0.619210722263589\n",
            "Accuracy: 0.6253164556962025\n",
            "Accuracy: 0.6314221891288161\n",
            "Accuracy: 0.6349962769918094\n",
            "Accuracy: 0.6329113924050633\n",
            "Accuracy: 0.6299329858525688\n",
            "Accuracy: 0.636783320923306\n",
            "Accuracy: 0.6336559940431868\n",
            "Accuracy: 0.6385703648548027\n",
            "Accuracy: 0.6284437825763217\n",
            "Accuracy: 0.6323157110945644\n",
            "Accuracy: 0.6354430379746835\n",
            "Accuracy: 0.6354430379746835\n",
            "Accuracy: 0.6317200297840655\n",
            "Accuracy: 0.6260610573343262\n",
            "Accuracy: 0.6323157110945644\n",
            "Accuracy: 0.6314221891288161\n",
            "Accuracy: 0.6336559940431868\n",
            "Accuracy: 0.6372300819061802\n",
            "Accuracy: 0.6279970215934475\n",
            "Accuracy: 0.6348473566641847\n",
            "Accuracy: 0.6285927029039464\n",
            "Accuracy: 0.6266567386448251\n",
            "Accuracy: 0.6382725241995533\n",
            "Accuracy: 0.6288905435591958\n",
            "Accuracy: 0.6257632166790766\n",
            "Accuracy: 0.6364854802680566\n",
            "Accuracy: 0.640506329113924\n",
            "Accuracy: 0.6358897989575577\n",
            "Accuracy: 0.6335070737155621\n",
            "Accuracy: 0.6360387192851824\n",
            "Accuracy: 0.633060312732688\n",
            "Accuracy: 0.6236783320923306\n",
            "Accuracy: 0.6315711094564408\n",
            "Accuracy: 0.6315711094564408\n",
            "Accuracy: 0.6332092330603127\n",
            "Accuracy: 0.6324646314221891\n",
            "Accuracy: 0.6332092330603127\n",
            "Accuracy: 0.6352941176470588\n",
            "Accuracy: 0.6262099776619509\n",
            "Accuracy: 0.632017870439315\n",
            "Accuracy: 0.6396128071481757\n",
            "Accuracy: 0.6278481012658228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy', np.mean(accuracies), np.std(accuracies))"
      ],
      "metadata": {
        "id": "mLEk5p_CzobB",
        "outputId": "362c14bc-9677-4673-8baa-362b0e9c8bbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.6310625465376024 0.003960000618947224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def uncertainty(softmax):\n",
        "  # Per example softmax with shape(num_examples, num_classes)\n",
        "  predictive_entropy = 0\n",
        "  single_pass_entropy = 0\n",
        "  for i in range(softmax.shape[1]):\n",
        "    # Sum over classes\n",
        "    predictive_entropy += -np.mean(softmax[:,i])*np.log(np.mean(softmax[:,i]))\n",
        "    single_pass_entropy += -softmax[:,i]*np.log(softmax[:,i])\n",
        "  single_pass_entropy = np.mean(single_pass_entropy)\n",
        "  mutual_info = predictive_entropy - single_pass_entropy\n",
        "  return predictive_entropy, single_pass_entropy, mutual_info"
      ],
      "metadata": {
        "id": "F_ixNIzAzqr6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictive_entropies = [] \n",
        "single_pass_entropies = []\n",
        "mutual_infos = []\n",
        "for i in range(softmax.shape[1]):\n",
        "  predictive_entropy, single_pass_entropy, mutual_info = uncertainty(softmax[:,i,:])\n",
        "  predictive_entropies.append(predictive_entropy)\n",
        "  single_pass_entropies.append(single_pass_entropy)\n",
        "  mutual_infos.append(mutual_info)\n",
        "print('Predictive entropy:', np.mean(predictive_entropies), np.std(predictive_entropies))\n",
        "print('Single pass entropy:', np.mean(single_pass_entropies), np.std(single_pass_entropies))\n",
        "print('Mutual info:', np.mean(mutual_infos), np.std(mutual_infos))"
      ],
      "metadata": {
        "id": "oUpRWlKzzqxo",
        "outputId": "08954f4a-a47b-4464-9bd3-8f1f104839bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictive entropy: 0.7548963319361538 0.30381212725454704\n",
            "Single pass entropy: 0.6599981 0.3030059\n",
            "Mutual info: 0.09489820944773557 0.04021800700096953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(predictive_entropies)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IHMK2hcCzq1w",
        "outputId": "e319c5c1-9318-4818-df21-68e1ae82f10b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(single_pass_entropies)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fqpZy0WuzwAA",
        "outputId": "d8b4bd28-e94d-4462-fa10-1aa7494d359e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(mutual_infos)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uc7XtvMuzwEw",
        "outputId": "64c0e231-6052-4087-c28a-1c90d4f9b886",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8uq47Tt1n1DI"
      },
      "outputs": [],
      "source": [
        "ds_test_Y1, info_test_Y1 = tfds.load(name='mlsst/Y1', split='test', with_info=True, as_supervised=True, batch_size=50)\n",
        "ds_test_Y1 = ds_test_Y1.map(normalize)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = []\n",
        "accuracies = []\n",
        "for _ in range(samples_iter):\n",
        "  y = []\n",
        "  yhat = []\n",
        "  logits = []\n",
        "  for (x_batch, y_batch) in ds_test_Y1:\n",
        "      s = new_model.predict(x_batch)\n",
        "      yhat.append(np.argmax(s, axis=-1))\n",
        "      y.append(y_batch)\n",
        "      logits.append(s)\n",
        "  y = np.concatenate(y)\n",
        "  yhat = np.concatenate(yhat)\n",
        "  softmax.append(np.concatenate(logits))\n",
        "  accuracies.append(np.mean(yhat == y))\n",
        "  print('Accuracy:', np.mean(yhat == y))\n",
        "softmax = np.array(softmax)"
      ],
      "metadata": {
        "id": "-BxpXGHuz59q",
        "outputId": "2b792c7e-7109-44bd-ca5f-bdad9cddcce8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5304542069992554\n",
            "Accuracy: 0.5306031273268801\n",
            "Accuracy: 0.5276247207743857\n",
            "Accuracy: 0.5268801191362621\n",
            "Accuracy: 0.5265822784810127\n",
            "Accuracy: 0.5271779597915115\n",
            "Accuracy: 0.5261355174981385\n",
            "Accuracy: 0.5259865971705138\n",
            "Accuracy: 0.5316455696202531\n",
            "Accuracy: 0.5256887565152644\n",
            "Accuracy: 0.5236038719285182\n",
            "Accuracy: 0.526433358153388\n",
            "Accuracy: 0.5255398361876397\n",
            "Accuracy: 0.5335815338793746\n",
            "Accuracy: 0.5300074460163813\n",
            "Accuracy: 0.5240506329113924\n",
            "Accuracy: 0.526433358153388\n",
            "Accuracy: 0.5216679076693969\n",
            "Accuracy: 0.5292628443782577\n",
            "Accuracy: 0.5289650037230081\n",
            "Accuracy: 0.5271779597915115\n",
            "Accuracy: 0.5273268801191363\n",
            "Accuracy: 0.5273268801191363\n",
            "Accuracy: 0.529113924050633\n",
            "Accuracy: 0.531198808637379\n",
            "Accuracy: 0.5288160833953834\n",
            "Accuracy: 0.5304542069992554\n",
            "Accuracy: 0.5265822784810127\n",
            "Accuracy: 0.5265822784810127\n",
            "Accuracy: 0.5280714817572598\n",
            "Accuracy: 0.5277736411020104\n",
            "Accuracy: 0.5288160833953834\n",
            "Accuracy: 0.5300074460163813\n",
            "Accuracy: 0.5265822784810127\n",
            "Accuracy: 0.5277736411020104\n",
            "Accuracy: 0.5250930752047654\n",
            "Accuracy: 0.5303052866716307\n",
            "Accuracy: 0.5234549516008935\n",
            "Accuracy: 0.5271779597915115\n",
            "Accuracy: 0.5347728965003723\n",
            "Accuracy: 0.5236038719285182\n",
            "Accuracy: 0.5243484735666418\n",
            "Accuracy: 0.530156366344006\n",
            "Accuracy: 0.5244973938942665\n",
            "Accuracy: 0.5319434102755026\n",
            "Accuracy: 0.5309009679821296\n",
            "Accuracy: 0.5303052866716307\n",
            "Accuracy: 0.5306031273268801\n",
            "Accuracy: 0.5230081906180194\n",
            "Accuracy: 0.5280714817572598\n",
            "Accuracy: 0.5300074460163813\n",
            "Accuracy: 0.5228592702903946\n",
            "Accuracy: 0.5230081906180194\n",
            "Accuracy: 0.5233060312732688\n",
            "Accuracy: 0.5314966492926284\n",
            "Accuracy: 0.5310498883097543\n",
            "Accuracy: 0.5280714817572598\n",
            "Accuracy: 0.5271779597915115\n",
            "Accuracy: 0.5314966492926284\n",
            "Accuracy: 0.5320923306031273\n",
            "Accuracy: 0.5286671630677587\n",
            "Accuracy: 0.523752792256143\n",
            "Accuracy: 0.5323901712583767\n",
            "Accuracy: 0.532241250930752\n",
            "Accuracy: 0.5286671630677587\n",
            "Accuracy: 0.5335815338793746\n",
            "Accuracy: 0.5249441548771407\n",
            "Accuracy: 0.5307520476545048\n",
            "Accuracy: 0.529113924050633\n",
            "Accuracy: 0.5280714817572598\n",
            "Accuracy: 0.5347728965003723\n",
            "Accuracy: 0.5271779597915115\n",
            "Accuracy: 0.528518242740134\n",
            "Accuracy: 0.5323901712583767\n",
            "Accuracy: 0.5317944899478779\n",
            "Accuracy: 0.5326880119136262\n",
            "Accuracy: 0.5216679076693969\n",
            "Accuracy: 0.5231571109456441\n",
            "Accuracy: 0.5298585256887565\n",
            "Accuracy: 0.526433358153388\n",
            "Accuracy: 0.5267311988086374\n",
            "Accuracy: 0.5300074460163813\n",
            "Accuracy: 0.5276247207743857\n",
            "Accuracy: 0.5307520476545048\n",
            "Accuracy: 0.5288160833953834\n",
            "Accuracy: 0.531198808637379\n",
            "Accuracy: 0.5265822784810127\n",
            "Accuracy: 0.5255398361876397\n",
            "Accuracy: 0.5346239761727476\n",
            "Accuracy: 0.5282204020848846\n",
            "Accuracy: 0.5395383469843633\n",
            "Accuracy: 0.5277736411020104\n",
            "Accuracy: 0.5253909158600149\n",
            "Accuracy: 0.5329858525688757\n",
            "Accuracy: 0.5304542069992554\n",
            "Accuracy: 0.5234549516008935\n",
            "Accuracy: 0.5326880119136262\n",
            "Accuracy: 0.5297096053611318\n",
            "Accuracy: 0.5267311988086374\n",
            "Accuracy: 0.531198808637379\n",
            "Accuracy: 0.5292628443782577\n",
            "Accuracy: 0.5225614296351452\n",
            "Accuracy: 0.5267311988086374\n",
            "Accuracy: 0.5367088607594936\n",
            "Accuracy: 0.5350707371556217\n",
            "Accuracy: 0.5314966492926284\n",
            "Accuracy: 0.5300074460163813\n",
            "Accuracy: 0.5298585256887565\n",
            "Accuracy: 0.5362620997766195\n",
            "Accuracy: 0.530156366344006\n",
            "Accuracy: 0.529113924050633\n",
            "Accuracy: 0.5313477289650037\n",
            "Accuracy: 0.5313477289650037\n",
            "Accuracy: 0.5252419955323901\n",
            "Accuracy: 0.5331347728965004\n",
            "Accuracy: 0.5344750558451229\n",
            "Accuracy: 0.5240506329113924\n",
            "Accuracy: 0.5300074460163813\n",
            "Accuracy: 0.526433358153388\n",
            "Accuracy: 0.523752792256143\n",
            "Accuracy: 0.531198808637379\n",
            "Accuracy: 0.5277736411020104\n",
            "Accuracy: 0.5289650037230081\n",
            "Accuracy: 0.5309009679821296\n",
            "Accuracy: 0.532241250930752\n",
            "Accuracy: 0.5304542069992554\n",
            "Accuracy: 0.5306031273268801\n",
            "Accuracy: 0.5288160833953834\n",
            "Accuracy: 0.527475800446761\n",
            "Accuracy: 0.5365599404318689\n",
            "Accuracy: 0.5253909158600149\n",
            "Accuracy: 0.5307520476545048\n",
            "Accuracy: 0.5329858525688757\n",
            "Accuracy: 0.5329858525688757\n",
            "Accuracy: 0.5286671630677587\n",
            "Accuracy: 0.5250930752047654\n",
            "Accuracy: 0.5295606850335071\n",
            "Accuracy: 0.5340282948622487\n",
            "Accuracy: 0.5280714817572598\n",
            "Accuracy: 0.5306031273268801\n",
            "Accuracy: 0.534921816827997\n",
            "Accuracy: 0.5268801191362621\n",
            "Accuracy: 0.5367088607594936\n",
            "Accuracy: 0.5270290394638868\n",
            "Accuracy: 0.5225614296351452\n",
            "Accuracy: 0.5331347728965004\n",
            "Accuracy: 0.5282204020848846\n",
            "Accuracy: 0.5276247207743857\n",
            "Accuracy: 0.5259865971705138\n",
            "Accuracy: 0.5335815338793746\n",
            "Accuracy: 0.5273268801191363\n",
            "Accuracy: 0.533879374534624\n",
            "Accuracy: 0.530156366344006\n",
            "Accuracy: 0.5286671630677587\n",
            "Accuracy: 0.526433358153388\n",
            "Accuracy: 0.5294117647058824\n",
            "Accuracy: 0.526433358153388\n",
            "Accuracy: 0.5249441548771407\n",
            "Accuracy: 0.5271779597915115\n",
            "Accuracy: 0.5313477289650037\n",
            "Accuracy: 0.528518242740134\n",
            "Accuracy: 0.5316455696202531\n",
            "Accuracy: 0.5295606850335071\n",
            "Accuracy: 0.5279225614296351\n",
            "Accuracy: 0.5240506329113924\n",
            "Accuracy: 0.5241995532390171\n",
            "Accuracy: 0.5239017125837677\n",
            "Accuracy: 0.5273268801191363\n",
            "Accuracy: 0.5265822784810127\n",
            "Accuracy: 0.5310498883097543\n",
            "Accuracy: 0.5249441548771407\n",
            "Accuracy: 0.5323901712583767\n",
            "Accuracy: 0.5276247207743857\n",
            "Accuracy: 0.5346239761727476\n",
            "Accuracy: 0.5256887565152644\n",
            "Accuracy: 0.5252419955323901\n",
            "Accuracy: 0.5334326135517499\n",
            "Accuracy: 0.5261355174981385\n",
            "Accuracy: 0.5271779597915115\n",
            "Accuracy: 0.5279225614296351\n",
            "Accuracy: 0.5250930752047654\n",
            "Accuracy: 0.5276247207743857\n",
            "Accuracy: 0.528518242740134\n",
            "Accuracy: 0.531198808637379\n",
            "Accuracy: 0.5282204020848846\n",
            "Accuracy: 0.5236038719285182\n",
            "Accuracy: 0.5316455696202531\n",
            "Accuracy: 0.5310498883097543\n",
            "Accuracy: 0.5286671630677587\n",
            "Accuracy: 0.5262844378257632\n",
            "Accuracy: 0.5331347728965004\n",
            "Accuracy: 0.5212211466865228\n",
            "Accuracy: 0.5288160833953834\n",
            "Accuracy: 0.5286671630677587\n",
            "Accuracy: 0.5343261355174982\n",
            "Accuracy: 0.5331347728965004\n",
            "Accuracy: 0.5218168279970216\n",
            "Accuracy: 0.529113924050633\n",
            "Accuracy: 0.5280714817572598\n",
            "Accuracy: 0.5346239761727476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy', np.mean(accuracies), np.std(accuracies))"
      ],
      "metadata": {
        "id": "B-98N5Nyz_yI",
        "outputId": "740f7abb-724f-4ac5-cad3-97245040dcad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.5287937453462398 0.003445623464021449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictive_entropies = [] \n",
        "single_pass_entropies = []\n",
        "mutual_infos = []\n",
        "for i in range(softmax.shape[1]):\n",
        "  predictive_entropy, single_pass_entropy, mutual_info = uncertainty(softmax[:,i,:])\n",
        "  predictive_entropies.append(predictive_entropy)\n",
        "  single_pass_entropies.append(single_pass_entropy)\n",
        "  mutual_infos.append(mutual_info)\n",
        "print('Predictive entropy:', np.mean(predictive_entropies), np.std(predictive_entropies))\n",
        "print('Single pass entropy:', np.mean(single_pass_entropies), np.std(single_pass_entropies))\n",
        "print('Mutual info:', np.mean(mutual_infos), np.std(mutual_infos))"
      ],
      "metadata": {
        "id": "KISh0ppIz_54",
        "outputId": "04b8f68f-1c00-43e7-81e4-55dd0ac49228",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictive entropy: 0.6539809428038921 0.3357729200996917\n",
            "Single pass entropy: 0.5589484 0.32511187\n",
            "Mutual info: 0.09503255430726877 0.04432828581332849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(predictive_entropies)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2UL8O_Vu0EJZ",
        "outputId": "7567ae6c-a8fe-4c4f-c231-4cde01867d52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(single_pass_entropies)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9B4lFmiE0ENT",
        "outputId": "02d8cbad-52e6-48c7-bb51-bb9d881e8c4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(mutual_infos)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rT3CuWhu0DtP",
        "outputId": "25ec3ef3-d18c-4526-9316-7e408b7763e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zMIDtW2E0ISq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "MLSST_MCD.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}