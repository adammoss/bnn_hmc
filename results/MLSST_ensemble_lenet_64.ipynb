{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLSST_ensemble_lenet_64.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adammoss/bnn_hmc/blob/main/results/MLSST_ensemble_lenet_64.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mhx8U_YuDQz8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import subprocess\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nruns = 10\n",
        "model = 'lenet'\n",
        "image_size = 64"
      ],
      "metadata": {
        "id": "NCiWvrIaio8t"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "58h11WCOit88",
        "outputId": "231a513e-7661-4e99-a827-6e4bc2172208",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Aug 24 15:49:15 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install astro-datasets --upgrade\n",
        "!pip install tensorflow_datasets --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFByvrIaDdbF",
        "outputId": "2e6173ac-2d83-4ecf-d52a-958679a82d9d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting astro-datasets\n",
            "  Downloading astro_datasets-0.0.10.tar.gz (12 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from astro-datasets) (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (from astro-datasets) (4.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from astro-datasets) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from astro-datasets) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->astro-datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->astro-datasets) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->astro-datasets) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (1.47.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (0.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (3.17.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (57.4.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (1.2.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (0.5.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (1.14.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (1.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (4.1.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (2.8.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (0.26.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->astro-datasets) (14.0.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->astro-datasets) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->astro-datasets) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (2022.6.15)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->astro-datasets) (3.2.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->astro-datasets) (0.10.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->astro-datasets) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->astro-datasets) (1.9.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->astro-datasets) (5.9.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->astro-datasets) (0.7.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->astro-datasets) (0.3.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->astro-datasets) (4.64.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets->astro-datasets) (1.56.4)\n",
            "Building wheels for collected packages: astro-datasets\n",
            "  Building wheel for astro-datasets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for astro-datasets: filename=astro_datasets-0.0.10-py3-none-any.whl size=15992 sha256=833e53b069f3c1a93660ebeaabbc0828f8c1fd5e8524df942c028a452a04b4c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/b2/9d/97c264f6addbd178fe1c8ff119617e1515cb8c0d0f220605cf\n",
            "Successfully built astro-datasets\n",
            "Installing collected packages: astro-datasets\n",
            "Successfully installed astro-datasets-0.0.10\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (4.64.0)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (3.17.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (5.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (4.1.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.3.5.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.9.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.7.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.2.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.23.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.15.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.10.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.24.3)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->tensorflow_datasets) (3.8.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.56.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.rmtree('bnn_hmc', ignore_errors=True)"
      ],
      "metadata": {
        "id": "eMgI24bqDevU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/adammoss/bnn_hmc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOWI7N14Dgid",
        "outputId": "06857370-0316-46b2-e787-47ecfafe7461"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bnn_hmc'...\n",
            "remote: Enumerating objects: 531, done.\u001b[K\n",
            "remote: Total 531 (delta 0), reused 0 (delta 0), pack-reused 531\u001b[K\n",
            "Receiving objects: 100% (531/531), 1001.61 KiB | 2.25 MiB/s, done.\n",
            "Resolving deltas: 100% (381/381), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade https://storage.googleapis.com/jax-releases/cuda111/jaxlib-0.1.65+cuda111-cp37-none-manylinux2010_x86_64.whl\n",
        "!pip install jax==0.2.12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK7oPA0-Ebee",
        "outputId": "6d42d948-c8eb-4f6f-88eb-ce144ebfd2a6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jaxlib==0.1.65+cuda111\n",
            "  Downloading https://storage.googleapis.com/jax-releases/cuda111/jaxlib-0.1.65+cuda111-cp37-none-manylinux2010_x86_64.whl (189.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 189.4 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jaxlib==0.1.65+cuda111) (1.2.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from jaxlib==0.1.65+cuda111) (1.7.3)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from jaxlib==0.1.65+cuda111) (2.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from jaxlib==0.1.65+cuda111) (1.21.6)\n",
            "Installing collected packages: jaxlib\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.3.14+cuda11.cudnn805\n",
            "    Uninstalling jaxlib-0.3.14+cuda11.cudnn805:\n",
            "      Successfully uninstalled jaxlib-0.3.14+cuda11.cudnn805\n",
            "Successfully installed jaxlib-0.1.65+cuda111\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jax==0.2.12\n",
            "  Downloading jax-0.2.12.tar.gz (590 kB)\n",
            "\u001b[K     |████████████████████████████████| 590 kB 15.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from jax==0.2.12) (1.21.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax==0.2.12) (1.2.0)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.7/dist-packages (from jax==0.2.12) (3.3.0)\n",
            "Building wheels for collected packages: jax\n",
            "  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.2.12-py3-none-any.whl size=682487 sha256=eac1674aa02a937b5cbe926a3d02ea8af8f94243e1b381741d8a7c850ea9a498\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/4d/e5/73eec5070b77f25664c67bd793d4eb97f41bbf9be7afafd15e\n",
            "Successfully built jax\n",
            "Installing collected packages: jax\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.3.14\n",
            "    Uninstalling jax-0.3.14:\n",
            "      Successfully uninstalled jax-0.3.14\n",
            "Successfully installed jax-0.2.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dm-haiku==0.0.5.dev0 optax==0.0.6 chex==0.0.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fEqU5bnEcDY",
        "outputId": "69e979f9-b351-48d2-8807-a0e91e504fcd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dm-haiku==0.0.5.dev0\n",
            "  Downloading dm_haiku-0.0.5.dev0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 16.4 MB/s \n",
            "\u001b[?25hCollecting optax==0.0.6\n",
            "  Downloading optax-0.0.6-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting chex==0.0.6\n",
            "  Downloading chex-0.0.6-py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 624 kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from dm-haiku==0.0.5.dev0) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from dm-haiku==0.0.5.dev0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from dm-haiku==0.0.5.dev0) (4.1.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from dm-haiku==0.0.5.dev0) (0.8.10)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax==0.0.6) (0.1.65+cuda111)\n",
            "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.7/dist-packages (from optax==0.0.6) (0.2.12)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex==0.0.6) (0.12.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex==0.0.6) (0.1.7)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax==0.0.6) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax==0.0.6) (2.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax==0.0.6) (1.7.3)\n",
            "Installing collected packages: chex, optax, dm-haiku\n",
            "Successfully installed chex-0.0.6 dm-haiku-0.0.5.dev0 optax-0.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(nruns):\n",
        " base_cmd = ['python3', 'bnn_hmc/scripts/run_sgd.py', '--seed=%s' % i, '--weight_decay=10', '--dir=runs/sgd/mlsst/%s/' % i, \n",
        " '--dataset_name=mlsst/Y10', '--model_name=%s' % model, '--init_step_size=3e-7', '--num_epochs=100', \n",
        " '--eval_freq=5', '--batch_size=100', '--save_freq=5', '--optimizer=SGD', '--image_size=%s' % image_size, \n",
        " '--subset_train_to=20000', '--scaling=asinh']\n",
        " train_cmd = base_cmd + ['--train_split=train', '--test_split=validation']\n",
        " eval_cmd = base_cmd + ['--eval_split=test']\n",
        " print(' '.join(train_cmd))\n",
        " p = subprocess.run(train_cmd, capture_output=True)\n",
        " print(p.stdout.decode('utf8'))\n",
        " p = subprocess.run(eval_cmd, capture_output=True)\n",
        " print(p.stdout.decode('utf8'))"
      ],
      "metadata": {
        "id": "tfEAjpNBi3yt",
        "outputId": "df06803a-1222-4933-cdc4-c0559038d92d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3 bnn_hmc/scripts/run_sgd.py --seed=0 --weight_decay=10 --dir=runs/sgd/mlsst/0/ --dataset_name=mlsst/Y10 --model_name=lenet --init_step_size=3e-7 --num_epochs=100 --eval_freq=5 --batch_size=100 --save_freq=5 --optimizer=SGD --image_size=64 --subset_train_to=20000 --scaling=asinh --train_split=train --test_split=validation\n",
            "JAX sees the following devices: [GpuDevice(id=0, process_index=0)]\n",
            "TF sees the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to ~/tensorflow_datasets/mlsst/Y10/1.0.0...\u001b[0m\n",
            "\u001b[1mDataset mlsst downloaded and prepared to ~/tensorflow_datasets/mlsst/Y10/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
            "Starting from random initialization with provided seed\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  0    4.7776            0.4237           0.4268      1.0731                                                         0.0000\n",
            "  1    0.4764                                                                                                        0.0000\n",
            "  2    0.4760                                                                                                        0.0000\n",
            "  3    0.4757                                                                                                        0.0000\n",
            "  4    0.4764                                                                                                        0.0000\n",
            "  5    0.4753            0.4237           0.4268      1.0731                                                         0.0000\n",
            "  6    0.4766                                                                                                        0.0000\n",
            "  7    0.4758                                                                                                        0.0000\n",
            "  8    0.4756                                                                                                        0.0000\n",
            "  9    0.4755                                                                                                        0.0000\n",
            " 10    0.4752            0.4237           0.4268      1.0733                                                         0.0000\n",
            " 11    0.4771                                                                                                        0.0000\n",
            " 12    0.4753                                                                                                        0.0000\n",
            " 13    0.4755                                                                                                        0.0000\n",
            " 14    0.4755                                                                                                        0.0000\n",
            " 15    0.4751            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 16    0.4764                                                                                                        0.0000\n",
            " 17    0.4757                                                                                                        0.0000\n",
            " 18    0.4762                                                                                                        0.0000\n",
            " 19    0.4768                                                                                                        0.0000\n",
            " 20    0.4761            0.4237           0.4268      1.0733                                                         0.0000\n",
            " 21    0.4755                                                                                                        0.0000\n",
            " 22    0.4766                                                                                                        0.0000\n",
            " 23    0.4766                                                                                                        0.0000\n",
            " 24    0.4765                                                                                                        0.0000\n",
            " 25    0.4763            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 26    0.4772                                                                                                        0.0000\n",
            " 27    0.4759                                                                                                        0.0000\n",
            " 28    0.4763                                                                                                        0.0000\n",
            " 29    0.4762                                                                                                        0.0000\n",
            " 30    0.4763            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 31    0.4777                                                                                                        0.0000\n",
            " 32    0.4763                                                                                                        0.0000\n",
            " 33    0.4764                                                                                                        0.0000\n",
            " 34    0.4765                                                                                                        0.0000\n",
            " 35    0.4767            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 36    0.4746                                                                                                        0.0000\n",
            " 37    0.4762                                                                                                        0.0000\n",
            " 38    0.4753                                                                                                        0.0000\n",
            " 39    0.4756                                                                                                        0.0000\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            " 40    0.4754            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 41    0.4759                                                                                                        0.0000\n",
            " 42    0.4752                                                                                                        0.0000\n",
            " 43    0.4748                                                                                                        0.0000\n",
            " 44    0.4755                                                                                                        0.0000\n",
            " 45    0.4756            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 46    0.4770                                                                                                        0.0000\n",
            " 47    0.4763                                                                                                        0.0000\n",
            " 48    0.4765                                                                                                        0.0000\n",
            " 49    0.4762                                                                                                        0.0000\n",
            " 50    0.4762            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 51    0.4758                                                                                                        0.0000\n",
            " 52    0.4753                                                                                                        0.0000\n",
            " 53    0.4751                                                                                                        0.0000\n",
            " 54    0.4756                                                                                                        0.0000\n",
            " 55    0.4753            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 56    0.4754                                                                                                        0.0000\n",
            " 57    0.4757                                                                                                        0.0000\n",
            " 58    0.4758                                                                                                        0.0000\n",
            " 59    0.4754                                                                                                        0.0000\n",
            " 60    0.4751            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 61    0.4762                                                                                                        0.0000\n",
            " 62    0.4751                                                                                                        0.0000\n",
            " 63    0.4754                                                                                                        0.0000\n",
            " 64    0.4757                                                                                                        0.0000\n",
            " 65    0.4754            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 66    0.4766                                                                                                        0.0000\n",
            " 67    0.4757                                                                                                        0.0000\n",
            " 68    0.4763                                                                                                        0.0000\n",
            " 69    0.4758                                                                                                        0.0000\n",
            " 70    0.4762            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 71    0.4767                                                                                                        0.0000\n",
            " 72    0.4748                                                                                                        0.0000\n",
            " 73    0.4751                                                                                                        0.0000\n",
            " 74    0.4763                                                                                                        0.0000\n",
            " 75    0.4774            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 76    0.4762                                                                                                        0.0000\n",
            " 77    0.4766                                                                                                        0.0000\n",
            " 78    0.4767                                                                                                        0.0000\n",
            " 79    0.4763                                                                                                        0.0000\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            " 80    0.4770            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 81    0.4783                                                                                                        0.0000\n",
            " 82    0.4751                                                                                                        0.0000\n",
            " 83    0.4751                                                                                                        0.0000\n",
            " 84    0.4760                                                                                                        0.0000\n",
            " 85    0.4759            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 86    0.4767                                                                                                        0.0000\n",
            " 87    0.4753                                                                                                        0.0000\n",
            " 88    0.4753                                                                                                        0.0000\n",
            " 89    0.4753                                                                                                        0.0000\n",
            " 90    0.4753            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 91    0.4753                                                                                                        0.0000\n",
            " 92    0.4746                                                                                                        0.0000\n",
            " 93    0.4748                                                                                                        0.0000\n",
            " 94    0.4760                                                                                                        0.0000\n",
            " 95    0.4756            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 96    0.4764                                                                                                        0.0000\n",
            " 97    0.4758                                                                                                        0.0000\n",
            " 98    0.4751                                                                                                        0.0000\n",
            " 99    0.4750            0.4237           0.4268      1.0730                                                         0.0000\n",
            "\n",
            "JAX sees the following devices: [GpuDevice(id=0, process_index=0)]\n",
            "TF sees the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "Continuing the run from the last saved checkpoint\n",
            "{'accuracy': DeviceArray(0.4263589, dtype=float32), 'nll': 1.0731498, 'ece': 0.0}\n",
            "\n",
            "python3 bnn_hmc/scripts/run_sgd.py --seed=1 --weight_decay=10 --dir=runs/sgd/mlsst/1/ --dataset_name=mlsst/Y10 --model_name=lenet --init_step_size=3e-7 --num_epochs=100 --eval_freq=5 --batch_size=100 --save_freq=5 --optimizer=SGD --image_size=64 --subset_train_to=20000 --scaling=asinh --train_split=train --test_split=validation\n",
            "JAX sees the following devices: [GpuDevice(id=0, process_index=0)]\n",
            "TF sees the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "Starting from random initialization with provided seed\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  0    4.6348            0.4237           0.4268      1.0732                                                         0.0000\n",
            "  1    0.4765                                                                                                        0.0000\n",
            "  2    0.4765                                                                                                        0.0000\n",
            "  3    0.4767                                                                                                        0.0000\n",
            "  4    0.4768                                                                                                        0.0000\n",
            "  5    0.4766            0.4237           0.4268      1.0730                                                         0.0000\n",
            "  6    0.4802                                                                                                        0.0000\n",
            "  7    0.4767                                                                                                        0.0000\n",
            "  8    0.4769                                                                                                        0.0000\n",
            "  9    0.4767                                                                                                        0.0000\n",
            " 10    0.4769            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 11    0.4778                                                                                                        0.0000\n",
            " 12    0.4768                                                                                                        0.0000\n",
            " 13    0.4772                                                                                                        0.0000\n",
            " 14    0.4772                                                                                                        0.0000\n",
            " 15    0.4772            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 16    0.4766                                                                                                        0.0000\n",
            " 17    0.4767                                                                                                        0.0000\n",
            " 18    0.4771                                                                                                        0.0000\n",
            " 19    0.4770                                                                                                        0.0000\n",
            " 20    0.4775            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 21    0.4768                                                                                                        0.0000\n",
            " 22    0.4768                                                                                                        0.0000\n",
            " 23    0.4764                                                                                                        0.0000\n",
            " 24    0.4763                                                                                                        0.0000\n",
            " 25    0.4760            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 26    0.4783                                                                                                        0.0000\n",
            " 27    0.4763                                                                                                        0.0000\n",
            " 28    0.4767                                                                                                        0.0000\n",
            " 29    0.4769                                                                                                        0.0000\n",
            " 30    0.4777            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 31    0.4772                                                                                                        0.0000\n",
            " 32    0.4772                                                                                                        0.0000\n",
            " 33    0.4774                                                                                                        0.0000\n",
            " 34    0.4776                                                                                                        0.0000\n",
            " 35    0.4776            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 36    0.4773                                                                                                        0.0000\n",
            " 37    0.4756                                                                                                        0.0000\n",
            " 38    0.4760                                                                                                        0.0000\n",
            " 39    0.4756                                                                                                        0.0000\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            " 40    0.4761            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 41    0.4773                                                                                                        0.0000\n",
            " 42    0.4755                                                                                                        0.0000\n",
            " 43    0.4750                                                                                                        0.0000\n",
            " 44    0.4764                                                                                                        0.0000\n",
            " 45    0.4756            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 46    0.4766                                                                                                        0.0000\n",
            " 47    0.4764                                                                                                        0.0000\n",
            " 48    0.4773                                                                                                        0.0000\n",
            " 49    0.4768                                                                                                        0.0000\n",
            " 50    0.4769            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 51    0.4773                                                                                                        0.0000\n",
            " 52    0.4758                                                                                                        0.0000\n",
            " 53    0.4755                                                                                                        0.0000\n",
            " 54    0.4756                                                                                                        0.0000\n",
            " 55    0.4753            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 56    0.4779                                                                                                        0.0000\n",
            " 57    0.4759                                                                                                        0.0000\n",
            " 58    0.4763                                                                                                        0.0000\n",
            " 59    0.4760                                                                                                        0.0000\n",
            " 60    0.4754            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 61    0.4800                                                                                                        0.0000\n",
            " 62    0.4760                                                                                                        0.0000\n",
            " 63    0.4763                                                                                                        0.0000\n",
            " 64    0.4852                                                                                                        0.0000\n",
            " 65    0.4761            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 66    0.4768                                                                                                        0.0000\n",
            " 67    0.4764                                                                                                        0.0000\n",
            " 68    0.4766                                                                                                        0.0000\n",
            " 69    0.4766                                                                                                        0.0000\n",
            " 70    0.4767            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 71    0.4779                                                                                                        0.0000\n",
            " 72    0.4754                                                                                                        0.0000\n",
            " 73    0.4754                                                                                                        0.0000\n",
            " 74    0.4758                                                                                                        0.0000\n",
            " 75    0.4762            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 76    0.4765                                                                                                        0.0000\n",
            " 77    0.4755                                                                                                        0.0000\n",
            " 78    0.4759                                                                                                        0.0000\n",
            " 79    0.4758                                                                                                        0.0000\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            " 80    0.4777            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 81    0.4774                                                                                                        0.0000\n",
            " 82    0.4754                                                                                                        0.0000\n",
            " 83    0.4758                                                                                                        0.0000\n",
            " 84    0.4763                                                                                                        0.0000\n",
            " 85    0.4761            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 86    0.4787                                                                                                        0.0000\n",
            " 87    0.4761                                                                                                        0.0000\n",
            " 88    0.4756                                                                                                        0.0000\n",
            " 89    0.4761                                                                                                        0.0000\n",
            " 90    0.4760            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 91    0.4770                                                                                                        0.0000\n",
            " 92    0.4752                                                                                                        0.0000\n",
            " 93    0.4755                                                                                                        0.0000\n",
            " 94    0.4759                                                                                                        0.0000\n",
            " 95    0.4762            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 96    0.4766                                                                                                        0.0000\n",
            " 97    0.4758                                                                                                        0.0000\n",
            " 98    0.4759                                                                                                        0.0000\n",
            " 99    0.4765            0.4237           0.4268      1.0730                                                         0.0000\n",
            "\n",
            "JAX sees the following devices: [GpuDevice(id=0, process_index=0)]\n",
            "TF sees the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "Continuing the run from the last saved checkpoint\n",
            "{'accuracy': DeviceArray(0.4263589, dtype=float32), 'nll': 1.0731512, 'ece': 0.0}\n",
            "\n",
            "python3 bnn_hmc/scripts/run_sgd.py --seed=2 --weight_decay=10 --dir=runs/sgd/mlsst/2/ --dataset_name=mlsst/Y10 --model_name=lenet --init_step_size=3e-7 --num_epochs=100 --eval_freq=5 --batch_size=100 --save_freq=5 --optimizer=SGD --image_size=64 --subset_train_to=20000 --scaling=asinh --train_split=train --test_split=validation\n",
            "JAX sees the following devices: [GpuDevice(id=0, process_index=0)]\n",
            "TF sees the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "Starting from random initialization with provided seed\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  0    4.6251            0.4237           0.4268      1.0733                                                         0.0000\n",
            "  1    0.5054                                                                                                        0.0000\n",
            "  2    0.4760                                                                                                        0.0000\n",
            "  3    0.4759                                                                                                        0.0000\n",
            "  4    0.4764                                                                                                        0.0000\n",
            "  5    0.4769            0.4237           0.4268      1.0728                                                         0.0000\n",
            "  6    0.4797                                                                                                        0.0000\n",
            "  7    0.4769                                                                                                        0.0000\n",
            "  8    0.4773                                                                                                        0.0000\n",
            "  9    0.4767                                                                                                        0.0000\n",
            " 10    0.4766            0.5862           0.5782      0.8814                                                         0.0000\n",
            " 11    0.4779                                                                                                        0.0000\n",
            " 12    0.4771                                                                                                        0.0000\n",
            " 13    0.4778                                                                                                        0.0000\n",
            " 14    0.4781                                                                                                        0.0000\n",
            " 15    0.4776            0.6014           0.5776      0.8876                                                         0.0000\n",
            " 16    0.4782                                                                                                        0.0000\n",
            " 17    0.4772                                                                                                        0.0000\n",
            " 18    0.4777                                                                                                        0.0000\n",
            " 19    0.4777                                                                                                        0.0000\n",
            " 20    0.4774            0.6363           0.6089      0.8940                                                         0.0000\n",
            " 21    0.4780                                                                                                        0.0000\n",
            " 22    0.4780                                                                                                        0.0000\n",
            " 23    0.4766                                                                                                        0.0000\n",
            " 24    0.4773                                                                                                        0.0000\n",
            " 25    0.4773            0.6946           0.6125      0.8918                                                         0.0000\n",
            " 26    0.4769                                                                                                        0.0000\n",
            " 27    0.4775                                                                                                        0.0000\n",
            " 28    0.4769                                                                                                        0.0000\n",
            " 29    0.4801                                                                                                        0.0000\n",
            " 30    0.4773            0.7021           0.5875      0.9116                                                         0.0000\n",
            " 31    0.4785                                                                                                        0.0000\n",
            " 32    0.4771                                                                                                        0.0000\n",
            " 33    0.4771                                                                                                        0.0000\n",
            " 34    0.4814                                                                                                        0.0000\n",
            " 35    0.4779            0.7517           0.6122      0.9549                                                         0.0000\n",
            " 36    0.4777                                                                                                        0.0000\n",
            " 37    0.4777                                                                                                        0.0000\n",
            " 38    0.4771                                                                                                        0.0000\n",
            " 39    0.4773                                                                                                        0.0000\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            " 40    0.4772            0.8028           0.6203      1.0373                                                         0.0000\n",
            " 41    0.4785                                                                                                        0.0000\n",
            " 42    0.4785                                                                                                        0.0000\n",
            " 43    0.4770                                                                                                        0.0000\n",
            " 44    0.4781                                                                                                        0.0000\n",
            " 45    0.4770            0.8822           0.6548      1.1568                                                         0.0000\n",
            " 46    0.4789                                                                                                        0.0000\n",
            " 47    0.4768                                                                                                        0.0000\n",
            " 48    0.4769                                                                                                        0.0000\n",
            " 49    0.4769                                                                                                        0.0000\n",
            " 50    0.4777            0.9426           0.6975      1.3545                                                         0.0000\n",
            " 51    0.4784                                                                                                        0.0000\n",
            " 52    0.4776                                                                                                        0.0000\n",
            " 53    0.4774                                                                                                        0.0000\n",
            " 54    0.4770                                                                                                        0.0000\n",
            " 55    0.4769            0.9695           0.7037      1.6008                                                         0.0000\n",
            " 56    0.4982                                                                                                        0.0000\n",
            " 57    0.4771                                                                                                        0.0000\n",
            " 58    0.4774                                                                                                        0.0000\n",
            " 59    0.4772                                                                                                        0.0000\n",
            " 60    0.4779            0.9867           0.7109      2.1111                                                         0.0000\n",
            " 61    0.4770                                                                                                        0.0000\n",
            " 62    0.4768                                                                                                        0.0000\n",
            " 63    0.4770                                                                                                        0.0000\n",
            " 64    0.4772                                                                                                        0.0000\n",
            " 65    0.4772            0.9997           0.7252      2.5860                                                         0.0000\n",
            " 66    0.4774                                                                                                        0.0000\n",
            " 67    0.4769                                                                                                        0.0000\n",
            " 68    0.4769                                                                                                        0.0000\n",
            " 69    0.4771                                                                                                        0.0000\n",
            " 70    0.4770            0.9998           0.7267      2.7010                                                         0.0000\n",
            " 71    0.4769                                                                                                        0.0000\n",
            " 72    0.4769                                                                                                        0.0000\n",
            " 73    0.4768                                                                                                        0.0000\n",
            " 74    0.4770                                                                                                        0.0000\n",
            " 75    0.4766            0.9998           0.7270      2.7337                                                         0.0000\n",
            " 76    0.4795                                                                                                        0.0000\n",
            " 77    0.4777                                                                                                        0.0000\n",
            " 78    0.4774                                                                                                        0.0000\n",
            " 79    0.4769                                                                                                        0.0000\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            " 80    0.4772            0.9998           0.7273      2.7427                                                         0.0000\n",
            " 81    0.4777                                                                                                        0.0000\n",
            " 82    0.4775                                                                                                        0.0000\n",
            " 83    0.4772                                                                                                        0.0000\n",
            " 84    0.4774                                                                                                        0.0000\n",
            " 85    0.4776            0.9998           0.7282      2.7473                                                         0.0000\n",
            " 86    0.4773                                                                                                        0.0000\n",
            " 87    0.4775                                                                                                        0.0000\n",
            " 88    0.4772                                                                                                        0.0000\n",
            " 89    0.4771                                                                                                        0.0000\n",
            " 90    0.4788            0.9998           0.7279      2.7500                                                         0.0000\n",
            " 91    0.4781                                                                                                        0.0000\n",
            " 92    0.4773                                                                                                        0.0000\n",
            " 93    0.4778                                                                                                        0.0000\n",
            " 94    0.4774                                                                                                        0.0000\n",
            " 95    0.4778            0.9998           0.7279      2.7505                                                         0.0000\n",
            " 96    0.4962                                                                                                        0.0000\n",
            " 97    0.4771                                                                                                        0.0000\n",
            " 98    0.4768                                                                                                        0.0000\n",
            " 99    0.4773            0.9998           0.7285      2.7507                                                         0.0000\n",
            "\n",
            "JAX sees the following devices: [GpuDevice(id=0, process_index=0)]\n",
            "TF sees the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "Continuing the run from the last saved checkpoint\n",
            "{'accuracy': DeviceArray(0.72583765, dtype=float32), 'nll': 2.843113, 'ece': DeviceArray(0.20497821, dtype=float32)}\n",
            "\n",
            "python3 bnn_hmc/scripts/run_sgd.py --seed=3 --weight_decay=10 --dir=runs/sgd/mlsst/3/ --dataset_name=mlsst/Y10 --model_name=lenet --init_step_size=3e-7 --num_epochs=100 --eval_freq=5 --batch_size=100 --save_freq=5 --optimizer=SGD --image_size=64 --subset_train_to=20000 --scaling=asinh --train_split=train --test_split=validation\n",
            "JAX sees the following devices: [GpuDevice(id=0, process_index=0)]\n",
            "TF sees the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "Starting from random initialization with provided seed\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  0    4.6445            0.4237           0.4268      1.0730                                                         0.0000\n",
            "  1    0.4761                                                                                                        0.0000\n",
            "  2    0.4768                                                                                                        0.0000\n",
            "  3    0.4766                                                                                                        0.0000\n",
            "  4    0.4768                                                                                                        0.0000\n",
            "  5    0.4769            0.4237           0.4268      1.0732                                                         0.0000\n",
            "  6    0.4766                                                                                                        0.0000\n",
            "  7    0.4765                                                                                                        0.0000\n",
            "  8    0.4770                                                                                                        0.0000\n",
            "  9    0.4772                                                                                                        0.0000\n",
            " 10    0.4768            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 11    0.4760                                                                                                        0.0000\n",
            " 12    0.4760                                                                                                        0.0000\n",
            " 13    0.4754                                                                                                        0.0000\n",
            " 14    0.4760                                                                                                        0.0000\n",
            " 15    0.4755            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 16    0.4765                                                                                                        0.0000\n",
            " 17    0.4764                                                                                                        0.0000\n",
            " 18    0.4767                                                                                                        0.0000\n",
            " 19    0.4771                                                                                                        0.0000\n",
            " 20    0.4767            0.4237           0.4268      1.0732                                                         0.0000\n",
            " 21    0.4768                                                                                                        0.0000\n",
            " 22    0.4775                                                                                                        0.0000\n",
            " 23    0.4764                                                                                                        0.0000\n",
            " 24    0.4772                                                                                                        0.0000\n",
            " 25    0.4764            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 26    0.4776                                                                                                        0.0000\n",
            " 27    0.4771                                                                                                        0.0000\n",
            " 28    0.4771                                                                                                        0.0000\n",
            " 29    0.4770                                                                                                        0.0000\n",
            " 30    0.4767            0.4237           0.4268      1.0732                                                         0.0000\n",
            " 31    0.4781                                                                                                        0.0000\n",
            " 32    0.4775                                                                                                        0.0000\n",
            " 33    0.4779                                                                                                        0.0000\n",
            " 34    0.4775                                                                                                        0.0000\n",
            " 35    0.4776            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 36    0.4783                                                                                                        0.0000\n",
            " 37    0.4769                                                                                                        0.0000\n",
            " 38    0.4779                                                                                                        0.0000\n",
            " 39    0.4768                                                                                                        0.0000\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            " 40    0.4769            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 41    0.4768                                                                                                        0.0000\n",
            " 42    0.4763                                                                                                        0.0000\n",
            " 43    0.4767                                                                                                        0.0000\n",
            " 44    0.4779                                                                                                        0.0000\n",
            " 45    0.4779            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 46    0.4784                                                                                                        0.0000\n",
            " 47    0.4763                                                                                                        0.0000\n",
            " 48    0.4764                                                                                                        0.0000\n",
            " 49    0.4761                                                                                                        0.0000\n",
            " 50    0.4768            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 51    0.4775                                                                                                        0.0000\n",
            " 52    0.4763                                                                                                        0.0000\n",
            " 53    0.4762                                                                                                        0.0000\n",
            " 54    0.4764                                                                                                        0.0000\n",
            " 55    0.4763            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 56    0.4770                                                                                                        0.0000\n",
            " 57    0.4769                                                                                                        0.0000\n",
            " 58    0.4774                                                                                                        0.0000\n",
            " 59    0.4776                                                                                                        0.0000\n",
            " 60    0.4769            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 61    0.4769                                                                                                        0.0000\n",
            " 62    0.4752                                                                                                        0.0000\n",
            " 63    0.4765                                                                                                        0.0000\n",
            " 64    0.4756                                                                                                        0.0000\n",
            " 65    0.4756            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 66    0.4802                                                                                                        0.0000\n",
            " 67    0.4776                                                                                                        0.0000\n",
            " 68    0.4775                                                                                                        0.0000\n",
            " 69    0.4780                                                                                                        0.0000\n",
            " 70    0.4773            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 71    0.4776                                                                                                        0.0000\n",
            " 72    0.4755                                                                                                        0.0000\n",
            " 73    0.4753                                                                                                        0.0000\n",
            " 74    0.4769                                                                                                        0.0000\n",
            " 75    0.4762            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 76    0.4771                                                                                                        0.0000\n",
            " 77    0.4760                                                                                                        0.0000\n",
            " 78    0.4756                                                                                                        0.0000\n",
            " 79    0.4763                                                                                                        0.0000\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            " 80    0.4761            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 81    0.4775                                                                                                        0.0000\n",
            " 82    0.4756                                                                                                        0.0000\n",
            " 83    0.4753                                                                                                        0.0000\n",
            " 84    0.4761                                                                                                        0.0000\n",
            " 85    0.4760            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 86    0.4769                                                                                                        0.0000\n",
            " 87    0.4757                                                                                                        0.0000\n",
            " 88    0.4758                                                                                                        0.0000\n",
            " 89    0.4762                                                                                                        0.0000\n",
            " 90    0.4756            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 91    0.4773                                                                                                        0.0000\n",
            " 92    0.4761                                                                                                        0.0000\n",
            " 93    0.4759                                                                                                        0.0000\n",
            " 94    0.4767                                                                                                        0.0000\n",
            " 95    0.4753            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 96    0.4762                                                                                                        0.0000\n",
            " 97    0.4761                                                                                                        0.0000\n",
            " 98    0.4764                                                                                                        0.0000\n",
            " 99    0.4763            0.4237           0.4268      1.0730                                                         0.0000\n",
            "\n",
            "JAX sees the following devices: [GpuDevice(id=0, process_index=0)]\n",
            "TF sees the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "Continuing the run from the last saved checkpoint\n",
            "{'accuracy': DeviceArray(0.4263589, dtype=float32), 'nll': 1.0731514, 'ece': 0.0}\n",
            "\n",
            "python3 bnn_hmc/scripts/run_sgd.py --seed=4 --weight_decay=10 --dir=runs/sgd/mlsst/4/ --dataset_name=mlsst/Y10 --model_name=lenet --init_step_size=3e-7 --num_epochs=100 --eval_freq=5 --batch_size=100 --save_freq=5 --optimizer=SGD --image_size=64 --subset_train_to=20000 --scaling=asinh --train_split=train --test_split=validation\n",
            "JAX sees the following devices: [GpuDevice(id=0, process_index=0)]\n",
            "TF sees the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "Starting from random initialization with provided seed\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  0    4.7112            0.4237           0.4268      1.0732                                                         0.0000\n",
            "  1    0.4769                                                                                                        0.0000\n",
            "  2    0.4775                                                                                                        0.0000\n",
            "  3    0.4774                                                                                                        0.0000\n",
            "  4    0.4772                                                                                                        0.0000\n",
            "  5    0.4772            0.4237           0.4268      1.0732                                                         0.0000\n",
            "  6    0.4790                                                                                                        0.0000\n",
            "  7    0.4771                                                                                                        0.0000\n",
            "  8    0.4767                                                                                                        0.0000\n",
            "  9    0.4773                                                                                                        0.0000\n",
            " 10    0.4766            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 11    0.4778                                                                                                        0.0000\n",
            " 12    0.4771                                                                                                        0.0000\n",
            " 13    0.4766                                                                                                        0.0000\n",
            " 14    0.4768                                                                                                        0.0000\n",
            " 15    0.4769            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 16    0.4779                                                                                                        0.0000\n",
            " 17    0.4781                                                                                                        0.0000\n",
            " 18    0.4775                                                                                                        0.0000\n",
            " 19    0.4775                                                                                                        0.0000\n",
            " 20    0.4780            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 21    0.4771                                                                                                        0.0000\n",
            " 22    0.4774                                                                                                        0.0000\n",
            " 23    0.4765                                                                                                        0.0000\n",
            " 24    0.4767                                                                                                        0.0000\n",
            " 25    0.4771            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 26    0.4786                                                                                                        0.0000\n",
            " 27    0.4775                                                                                                        0.0000\n",
            " 28    0.4777                                                                                                        0.0000\n",
            " 29    0.4772                                                                                                        0.0000\n",
            " 30    0.4769            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 31    0.4780                                                                                                        0.0000\n",
            " 32    0.4774                                                                                                        0.0000\n",
            " 33    0.4774                                                                                                        0.0000\n",
            " 34    0.4768                                                                                                        0.0000\n",
            " 35    0.4773            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 36    0.4785                                                                                                        0.0000\n",
            " 37    0.4768                                                                                                        0.0000\n",
            " 38    0.4769                                                                                                        0.0000\n",
            " 39    0.4764                                                                                                        0.0000\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            " 40    0.4768            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 41    0.4774                                                                                                        0.0000\n",
            " 42    0.4782                                                                                                        0.0000\n",
            " 43    0.4773                                                                                                        0.0000\n",
            " 44    0.4769                                                                                                        0.0000\n",
            " 45    0.4773            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 46    0.4797                                                                                                        0.0000\n",
            " 47    0.4776                                                                                                        0.0000\n",
            " 48    0.4778                                                                                                        0.0000\n",
            " 49    0.4778                                                                                                        0.0000\n",
            " 50    0.4782            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 51    0.4782                                                                                                        0.0000\n",
            " 52    0.4770                                                                                                        0.0000\n",
            " 53    0.4764                                                                                                        0.0000\n",
            " 54    0.4767                                                                                                        0.0000\n",
            " 55    0.4761            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 56    0.4782                                                                                                        0.0000\n",
            " 57    0.4768                                                                                                        0.0000\n",
            " 58    0.4765                                                                                                        0.0000\n",
            " 59    0.4768                                                                                                        0.0000\n",
            " 60    0.4765            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 61    0.4780                                                                                                        0.0000\n",
            " 62    0.4764                                                                                                        0.0000\n",
            " 63    0.4758                                                                                                        0.0000\n",
            " 64    0.4767                                                                                                        0.0000\n",
            " 65    0.4757            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 66    0.4782                                                                                                        0.0000\n",
            " 67    0.4768                                                                                                        0.0000\n",
            " 68    0.4769                                                                                                        0.0000\n",
            " 69    0.4766                                                                                                        0.0000\n",
            " 70    0.4763            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 71    0.4779                                                                                                        0.0000\n",
            " 72    0.4769                                                                                                        0.0000\n",
            " 73    0.4766                                                                                                        0.0000\n",
            " 74    0.4765                                                                                                        0.0000\n",
            " 75    0.4766            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 76    0.4781                                                                                                        0.0000\n",
            " 77    0.4762                                                                                                        0.0000\n",
            " 78    0.4767                                                                                                        0.0000\n",
            " 79    0.4766                                                                                                        0.0000\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            " 80    0.4769            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 81    0.4783                                                                                                        0.0000\n",
            " 82    0.4767                                                                                                        0.0000\n",
            " 83    0.4767                                                                                                        0.0000\n",
            " 84    0.4769                                                                                                        0.0000\n",
            " 85    0.4768            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 86    0.4784                                                                                                        0.0000\n",
            " 87    0.4764                                                                                                        0.0000\n",
            " 88    0.4764                                                                                                        0.0000\n",
            " 89    0.4758                                                                                                        0.0000\n",
            " 90    0.4764            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 91    0.4779                                                                                                        0.0000\n",
            " 92    0.4776                                                                                                        0.0000\n",
            " 93    0.4772                                                                                                        0.0000\n",
            " 94    0.4773                                                                                                        0.0000\n",
            " 95    0.4776            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 96    0.4777                                                                                                        0.0000\n",
            " 97    0.4769                                                                                                        0.0000\n",
            " 98    0.4769                                                                                                        0.0000\n",
            " 99    0.4771            0.4237           0.4268      1.0730                                                         0.0000\n",
            "\n",
            "JAX sees the following devices: [GpuDevice(id=0, process_index=0)]\n",
            "TF sees the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "Continuing the run from the last saved checkpoint\n",
            "{'accuracy': DeviceArray(0.4263589, dtype=float32), 'nll': 1.0731514, 'ece': 0.0}\n",
            "\n",
            "python3 bnn_hmc/scripts/run_sgd.py --seed=5 --weight_decay=10 --dir=runs/sgd/mlsst/5/ --dataset_name=mlsst/Y10 --model_name=lenet --init_step_size=3e-7 --num_epochs=100 --eval_freq=5 --batch_size=100 --save_freq=5 --optimizer=SGD --image_size=64 --subset_train_to=20000 --scaling=asinh --train_split=train --test_split=validation\n",
            "JAX sees the following devices: [GpuDevice(id=0, process_index=0)]\n",
            "TF sees the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "Starting from random initialization with provided seed\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  0    4.6764            0.4237           0.4268      1.0732                                                         0.0000\n",
            "  1    0.4759                                                                                                        0.0000\n",
            "  2    0.4775                                                                                                        0.0000\n",
            "  3    0.4771                                                                                                        0.0000\n",
            "  4    0.4770                                                                                                        0.0000\n",
            "  5    0.4764            0.4237           0.4268      1.0731                                                         0.0000\n",
            "  6    0.4779                                                                                                        0.0000\n",
            "  7    0.4766                                                                                                        0.0000\n",
            "  8    0.4763                                                                                                        0.0000\n",
            "  9    0.4763                                                                                                        0.0000\n",
            " 10    0.4762            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 11    0.4767                                                                                                        0.0000\n",
            " 12    0.4770                                                                                                        0.0000\n",
            " 13    0.4762                                                                                                        0.0000\n",
            " 14    0.4768                                                                                                        0.0000\n",
            " 15    0.4771            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 16    0.4776                                                                                                        0.0000\n",
            " 17    0.4766                                                                                                        0.0000\n",
            " 18    0.4795                                                                                                        0.0000\n",
            " 19    0.4763                                                                                                        0.0000\n",
            " 20    0.4766            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 21    0.4784                                                                                                        0.0000\n",
            " 22    0.4758                                                                                                        0.0000\n",
            " 23    0.4766                                                                                                        0.0000\n",
            " 24    0.4764                                                                                                        0.0000\n",
            " 25    0.4770            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 26    0.4775                                                                                                        0.0000\n",
            " 27    0.4773                                                                                                        0.0000\n",
            " 28    0.4773                                                                                                        0.0000\n",
            " 29    0.4771                                                                                                        0.0000\n",
            " 30    0.4775            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 31    0.4772                                                                                                        0.0000\n",
            " 32    0.4767                                                                                                        0.0000\n",
            " 33    0.4765                                                                                                        0.0000\n",
            " 34    0.4785                                                                                                        0.0000\n",
            " 35    0.4771            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 36    0.4786                                                                                                        0.0000\n",
            " 37    0.4771                                                                                                        0.0000\n",
            " 38    0.4769                                                                                                        0.0000\n",
            " 39    0.4769                                                                                                        0.0000\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            " 40    0.4765            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 41    0.4774                                                                                                        0.0000\n",
            " 42    0.4767                                                                                                        0.0000\n",
            " 43    0.4763                                                                                                        0.0000\n",
            " 44    0.4768                                                                                                        0.0000\n",
            " 45    0.4761            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 46    0.4780                                                                                                        0.0000\n",
            " 47    0.4766                                                                                                        0.0000\n",
            " 48    0.4763                                                                                                        0.0000\n",
            " 49    0.4767                                                                                                        0.0000\n",
            " 50    0.4768            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 51    0.4778                                                                                                        0.0000\n",
            " 52    0.4772                                                                                                        0.0000\n",
            " 53    0.4765                                                                                                        0.0000\n",
            " 54    0.4761                                                                                                        0.0000\n",
            " 55    0.4763            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 56    0.4784                                                                                                        0.0000\n",
            " 57    0.4770                                                                                                        0.0000\n",
            " 58    0.4768                                                                                                        0.0000\n",
            " 59    0.4767                                                                                                        0.0000\n",
            " 60    0.4766            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 61    0.4772                                                                                                        0.0000\n",
            " 62    0.4771                                                                                                        0.0000\n",
            " 63    0.4769                                                                                                        0.0000\n",
            " 64    0.4769                                                                                                        0.0000\n",
            " 65    0.4768            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 66    0.4780                                                                                                        0.0000\n",
            " 67    0.4757                                                                                                        0.0000\n",
            " 68    0.4763                                                                                                        0.0000\n",
            " 69    0.4768                                                                                                        0.0000\n",
            " 70    0.4762            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 71    0.4776                                                                                                        0.0000\n",
            " 72    0.4768                                                                                                        0.0000\n",
            " 73    0.4771                                                                                                        0.0000\n",
            " 74    0.4766                                                                                                        0.0000\n",
            " 75    0.4767            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 76    0.4775                                                                                                        0.0000\n",
            " 77    0.4768                                                                                                        0.0000\n",
            " 78    0.4763                                                                                                        0.0000\n",
            " 79    0.4770                                                                                                        0.0000\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            " 80    0.4770            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 81    0.4787                                                                                                        0.0000\n",
            " 82    0.4770                                                                                                        0.0000\n",
            " 83    0.4773                                                                                                        0.0000\n",
            " 84    0.4782                                                                                                        0.0000\n",
            " 85    0.4769            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 86    0.4790                                                                                                        0.0000\n",
            " 87    0.4771                                                                                                        0.0000\n",
            " 88    0.4771                                                                                                        0.0000\n",
            " 89    0.4775                                                                                                        0.0000\n",
            " 90    0.4770            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 91    0.4777                                                                                                        0.0000\n",
            " 92    0.4770                                                                                                        0.0000\n",
            " 93    0.4772                                                                                                        0.0000\n",
            " 94    0.4775                                                                                                        0.0000\n",
            " 95    0.4769            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 96    0.4778                                                                                                        0.0000\n",
            " 97    0.4780                                                                                                        0.0000\n",
            " 98    0.4764                                                                                                        0.0000\n",
            " 99    0.4779            0.4237           0.4268      1.0730                                                         0.0000\n",
            "\n",
            "JAX sees the following devices: [GpuDevice(id=0, process_index=0)]\n",
            "TF sees the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "Continuing the run from the last saved checkpoint\n",
            "{'accuracy': DeviceArray(0.4263589, dtype=float32), 'nll': 1.0731511, 'ece': 0.0}\n",
            "\n",
            "python3 bnn_hmc/scripts/run_sgd.py --seed=6 --weight_decay=10 --dir=runs/sgd/mlsst/6/ --dataset_name=mlsst/Y10 --model_name=lenet --init_step_size=3e-7 --num_epochs=100 --eval_freq=5 --batch_size=100 --save_freq=5 --optimizer=SGD --image_size=64 --subset_train_to=20000 --scaling=asinh --train_split=train --test_split=validation\n",
            "JAX sees the following devices: [GpuDevice(id=0, process_index=0)]\n",
            "TF sees the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "Starting from random initialization with provided seed\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  0    4.5755            0.4237           0.4268      1.0736                                                         0.0000\n",
            "  1    0.4974                                                                                                        0.0000\n",
            "  2    0.4761                                                                                                        0.0000\n",
            "  3    0.4760                                                                                                        0.0000\n",
            "  4    0.4757                                                                                                        0.0000\n",
            "  5    0.4756            0.4237           0.4268      1.0732                                                         0.0000\n",
            "  6    0.4759                                                                                                        0.0000\n",
            "  7    0.4761                                                                                                        0.0000\n",
            "  8    0.4762                                                                                                        0.0000\n",
            "  9    0.4759                                                                                                        0.0000\n",
            " 10    0.4763            0.4237           0.4268      1.0733                                                         0.0000\n",
            " 11    0.4761                                                                                                        0.0000\n",
            " 12    0.4766                                                                                                        0.0000\n",
            " 13    0.4764                                                                                                        0.0000\n",
            " 14    0.4772                                                                                                        0.0000\n",
            " 15    0.4766            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 16    0.4762                                                                                                        0.0000\n",
            " 17    0.4754                                                                                                        0.0000\n",
            " 18    0.4766                                                                                                        0.0000\n",
            " 19    0.4758                                                                                                        0.0000\n",
            " 20    0.4761            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 21    0.4767                                                                                                        0.0000\n",
            " 22    0.4764                                                                                                        0.0000\n",
            " 23    0.4760                                                                                                        0.0000\n",
            " 24    0.4766                                                                                                        0.0000\n",
            " 25    0.4760            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 26    0.4778                                                                                                        0.0000\n",
            " 27    0.4766                                                                                                        0.0000\n",
            " 28    0.4762                                                                                                        0.0000\n",
            " 29    0.4762                                                                                                        0.0000\n",
            " 30    0.4763            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 31    0.4773                                                                                                        0.0000\n",
            " 32    0.4757                                                                                                        0.0000\n",
            " 33    0.4758                                                                                                        0.0000\n",
            " 34    0.4760                                                                                                        0.0000\n",
            " 35    0.4760            0.4237           0.4268      1.0732                                                         0.0000\n",
            " 36    0.4764                                                                                                        0.0000\n",
            " 37    0.4759                                                                                                        0.0000\n",
            " 38    0.4766                                                                                                        0.0000\n",
            " 39    0.4758                                                                                                        0.0000\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            " 40    0.4763            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 41    0.4764                                                                                                        0.0000\n",
            " 42    0.4749                                                                                                        0.0000\n",
            " 43    0.4747                                                                                                        0.0000\n",
            " 44    0.4761                                                                                                        0.0000\n",
            " 45    0.4757            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 46    0.4766                                                                                                        0.0000\n",
            " 47    0.4755                                                                                                        0.0000\n",
            " 48    0.4761                                                                                                        0.0000\n",
            " 49    0.4756                                                                                                        0.0000\n",
            " 50    0.4759            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 51    0.4773                                                                                                        0.0000\n",
            " 52    0.4758                                                                                                        0.0000\n",
            " 53    0.4755                                                                                                        0.0000\n",
            " 54    0.4755                                                                                                        0.0000\n",
            " 55    0.4759            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 56    0.4765                                                                                                        0.0000\n",
            " 57    0.4763                                                                                                        0.0000\n",
            " 58    0.4763                                                                                                        0.0000\n",
            " 59    0.4753                                                                                                        0.0000\n",
            " 60    0.4759            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 61    0.4762                                                                                                        0.0000\n",
            " 62    0.4746                                                                                                        0.0000\n",
            " 63    0.4761                                                                                                        0.0000\n",
            " 64    0.4756                                                                                                        0.0000\n",
            " 65    0.4754            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 66    0.4783                                                                                                        0.0000\n",
            " 67    0.4761                                                                                                        0.0000\n",
            " 68    0.4764                                                                                                        0.0000\n",
            " 69    0.4755                                                                                                        0.0000\n",
            " 70    0.4760            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 71    0.4768                                                                                                        0.0000\n",
            " 72    0.4760                                                                                                        0.0000\n",
            " 73    0.4758                                                                                                        0.0000\n",
            " 74    0.4758                                                                                                        0.0000\n",
            " 75    0.4758            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 76    0.4776                                                                                                        0.0000\n",
            " 77    0.4756                                                                                                        0.0000\n",
            " 78    0.4758                                                                                                        0.0000\n",
            " 79    0.4749                                                                                                        0.0000\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            " 80    0.4759            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 81    0.4778                                                                                                        0.0000\n",
            " 82    0.4747                                                                                                        0.0000\n",
            " 83    0.4753                                                                                                        0.0000\n",
            " 84    0.4758                                                                                                        0.0000\n",
            " 85    0.4758            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 86    0.4778                                                                                                        0.0000\n",
            " 87    0.4754                                                                                                        0.0000\n",
            " 88    0.4751                                                                                                        0.0000\n",
            " 89    0.4754                                                                                                        0.0000\n",
            " 90    0.4758            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 91    0.4762                                                                                                        0.0000\n",
            " 92    0.4751                                                                                                        0.0000\n",
            " 93    0.4767                                                                                                        0.0000\n",
            " 94    0.4757                                                                                                        0.0000\n",
            " 95    0.4760            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 96    0.4774                                                                                                        0.0000\n",
            " 97    0.4756                                                                                                        0.0000\n",
            " 98    0.4757                                                                                                        0.0000\n",
            " 99    0.4751            0.4237           0.4268      1.0730                                                         0.0000\n",
            "\n",
            "JAX sees the following devices: [GpuDevice(id=0, process_index=0)]\n",
            "TF sees the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "Continuing the run from the last saved checkpoint\n",
            "{'accuracy': DeviceArray(0.4263589, dtype=float32), 'nll': 1.0731514, 'ece': 0.0}\n",
            "\n",
            "python3 bnn_hmc/scripts/run_sgd.py --seed=7 --weight_decay=10 --dir=runs/sgd/mlsst/7/ --dataset_name=mlsst/Y10 --model_name=lenet --init_step_size=3e-7 --num_epochs=100 --eval_freq=5 --batch_size=100 --save_freq=5 --optimizer=SGD --image_size=64 --subset_train_to=20000 --scaling=asinh --train_split=train --test_split=validation\n",
            "JAX sees the following devices: [GpuDevice(id=0, process_index=0)]\n",
            "TF sees the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "Starting from random initialization with provided seed\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  0    4.6188            0.4237           0.4268      1.0731                                                         0.0000\n",
            "  1    0.4763                                                                                                        0.0000\n",
            "  2    0.4769                                                                                                        0.0000\n",
            "  3    0.4769                                                                                                        0.0000\n",
            "  4    0.4769                                                                                                        0.0000\n",
            "  5    0.4770            0.4237           0.4268      1.0730                                                         0.0000\n",
            "  6    0.4773                                                                                                        0.0000\n",
            "  7    0.4771                                                                                                        0.0000\n",
            "  8    0.4770                                                                                                        0.0000\n",
            "  9    0.4775                                                                                                        0.0000\n",
            " 10    0.4776            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 11    0.4779                                                                                                        0.0000\n",
            " 12    0.4776                                                                                                        0.0000\n",
            " 13    0.4771                                                                                                        0.0000\n",
            " 14    0.4769                                                                                                        0.0000\n",
            " 15    0.4767            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 16    0.4770                                                                                                        0.0000\n",
            " 17    0.4763                                                                                                        0.0000\n",
            " 18    0.4766                                                                                                        0.0000\n",
            " 19    0.4766                                                                                                        0.0000\n",
            " 20    0.4764            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 21    0.4778                                                                                                        0.0000\n",
            " 22    0.4772                                                                                                        0.0000\n",
            " 23    0.4770                                                                                                        0.0000\n",
            " 24    0.4769                                                                                                        0.0000\n",
            " 25    0.4766            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 26    0.4782                                                                                                        0.0000\n",
            " 27    0.4778                                                                                                        0.0000\n",
            " 28    0.4779                                                                                                        0.0000\n",
            " 29    0.4779                                                                                                        0.0000\n",
            " 30    0.4779            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 31    0.4783                                                                                                        0.0000\n",
            " 32    0.4769                                                                                                        0.0000\n",
            " 33    0.4771                                                                                                        0.0000\n",
            " 34    0.4771                                                                                                        0.0000\n",
            " 35    0.4769            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 36    0.4782                                                                                                        0.0000\n",
            " 37    0.4766                                                                                                        0.0000\n",
            " 38    0.4771                                                                                                        0.0000\n",
            " 39    0.4768                                                                                                        0.0000\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            " 40    0.4766            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 41    0.4778                                                                                                        0.0000\n",
            " 42    0.4770                                                                                                        0.0000\n",
            " 43    0.4767                                                                                                        0.0000\n",
            " 44    0.4785                                                                                                        0.0000\n",
            " 45    0.4777            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 46    0.4773                                                                                                        0.0000\n",
            " 47    0.4764                                                                                                        0.0000\n",
            " 48    0.4770                                                                                                        0.0000\n",
            " 49    0.4761                                                                                                        0.0000\n",
            " 50    0.4765            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 51    0.4783                                                                                                        0.0000\n",
            " 52    0.4768                                                                                                        0.0000\n",
            " 53    0.4764                                                                                                        0.0000\n",
            " 54    0.4773                                                                                                        0.0000\n",
            " 55    0.4768            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 56    0.4786                                                                                                        0.0000\n",
            " 57    0.4770                                                                                                        0.0000\n",
            " 58    0.4774                                                                                                        0.0000\n",
            " 59    0.4769                                                                                                        0.0000\n",
            " 60    0.4770            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 61    0.4775                                                                                                        0.0000\n",
            " 62    0.4759                                                                                                        0.0000\n",
            " 63    0.4769                                                                                                        0.0000\n",
            " 64    0.4765                                                                                                        0.0000\n",
            " 65    0.4767            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 66    0.4777                                                                                                        0.0000\n",
            " 67    0.4773                                                                                                        0.0000\n",
            " 68    0.4774                                                                                                        0.0000\n",
            " 69    0.4771                                                                                                        0.0000\n",
            " 70    0.4775            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 71    0.4769                                                                                                        0.0000\n",
            " 72    0.4765                                                                                                        0.0000\n",
            " 73    0.4762                                                                                                        0.0000\n",
            " 74    0.4762                                                                                                        0.0000\n",
            " 75    0.4761            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 76    0.4781                                                                                                        0.0000\n",
            " 77    0.4772                                                                                                        0.0000\n",
            " 78    0.4778                                                                                                        0.0000\n",
            " 79    0.4771                                                                                                        0.0000\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            " 80    0.4780            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 81    0.4779                                                                                                        0.0000\n",
            " 82    0.4764                                                                                                        0.0000\n",
            " 83    0.4760                                                                                                        0.0000\n",
            " 84    0.4763                                                                                                        0.0000\n",
            " 85    0.4765            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 86    0.4773                                                                                                        0.0000\n",
            " 87    0.4758                                                                                                        0.0000\n",
            " 88    0.4767                                                                                                        0.0000\n",
            " 89    0.4762                                                                                                        0.0000\n",
            " 90    0.4775            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 91    0.4778                                                                                                        0.0000\n",
            " 92    0.4762                                                                                                        0.0000\n",
            " 93    0.4761                                                                                                        0.0000\n",
            " 94    0.4765                                                                                                        0.0000\n",
            " 95    0.4767            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 96    0.4786                                                                                                        0.0000\n",
            " 97    0.4759                                                                                                        0.0000\n",
            " 98    0.4761                                                                                                        0.0000\n",
            " 99    0.4759            0.4237           0.4268      1.0730                                                         0.0000\n",
            "\n",
            "JAX sees the following devices: [GpuDevice(id=0, process_index=0)]\n",
            "TF sees the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "Continuing the run from the last saved checkpoint\n",
            "{'accuracy': DeviceArray(0.4263589, dtype=float32), 'nll': 1.073151, 'ece': 0.0}\n",
            "\n",
            "python3 bnn_hmc/scripts/run_sgd.py --seed=8 --weight_decay=10 --dir=runs/sgd/mlsst/8/ --dataset_name=mlsst/Y10 --model_name=lenet --init_step_size=3e-7 --num_epochs=100 --eval_freq=5 --batch_size=100 --save_freq=5 --optimizer=SGD --image_size=64 --subset_train_to=20000 --scaling=asinh --train_split=train --test_split=validation\n",
            "JAX sees the following devices: [GpuDevice(id=0, process_index=0)]\n",
            "TF sees the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "Starting from random initialization with provided seed\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  0    4.7176            0.4237           0.4268      1.0730                                                         0.0000\n",
            "  1    0.4758                                                                                                        0.0000\n",
            "  2    0.4767                                                                                                        0.0000\n",
            "  3    0.4778                                                                                                        0.0000\n",
            "  4    0.4763                                                                                                        0.0000\n",
            "  5    0.4770            0.4237           0.4268      1.0732                                                         0.0000\n",
            "  6    0.4758                                                                                                        0.0000\n",
            "  7    0.4766                                                                                                        0.0000\n",
            "  8    0.4768                                                                                                        0.0000\n",
            "  9    0.4770                                                                                                        0.0000\n",
            " 10    0.4765            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 11    0.4768                                                                                                        0.0000\n",
            " 12    0.4760                                                                                                        0.0000\n",
            " 13    0.4762                                                                                                        0.0000\n",
            " 14    0.4756                                                                                                        0.0000\n",
            " 15    0.4750            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 16    0.4763                                                                                                        0.0000\n",
            " 17    0.4757                                                                                                        0.0000\n",
            " 18    0.4754                                                                                                        0.0000\n",
            " 19    0.4770                                                                                                        0.0000\n",
            " 20    0.4766            0.4237           0.4268      1.0732                                                         0.0000\n",
            " 21    0.4770                                                                                                        0.0000\n",
            " 22    0.4765                                                                                                        0.0000\n",
            " 23    0.4765                                                                                                        0.0000\n",
            " 24    0.4768                                                                                                        0.0000\n",
            " 25    0.4767            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 26    0.4768                                                                                                        0.0000\n",
            " 27    0.4765                                                                                                        0.0000\n",
            " 28    0.4764                                                                                                        0.0000\n",
            " 29    0.4765                                                                                                        0.0000\n",
            " 30    0.4766            0.4237           0.4268      1.0732                                                         0.0000\n",
            " 31    0.4784                                                                                                        0.0000\n",
            " 32    0.4765                                                                                                        0.0000\n",
            " 33    0.4766                                                                                                        0.0000\n",
            " 34    0.4763                                                                                                        0.0000\n",
            " 35    0.4761            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 36    0.4778                                                                                                        0.0000\n",
            " 37    0.4763                                                                                                        0.0000\n",
            " 38    0.4765                                                                                                        0.0000\n",
            " 39    0.4761                                                                                                        0.0000\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            " 40    0.4759            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 41    0.4776                                                                                                        0.0000\n",
            " 42    0.4764                                                                                                        0.0000\n",
            " 43    0.4765                                                                                                        0.0000\n",
            " 44    0.4763                                                                                                        0.0000\n",
            " 45    0.4761            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 46    0.4779                                                                                                        0.0000\n",
            " 47    0.4767                                                                                                        0.0000\n",
            " 48    0.4763                                                                                                        0.0000\n",
            " 49    0.4763                                                                                                        0.0000\n",
            " 50    0.4764            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 51    0.4777                                                                                                        0.0000\n",
            " 52    0.4768                                                                                                        0.0000\n",
            " 53    0.4779                                                                                                        0.0000\n",
            " 54    0.4767                                                                                                        0.0000\n",
            " 55    0.4767            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 56    0.4765                                                                                                        0.0000\n",
            " 57    0.4759                                                                                                        0.0000\n",
            " 58    0.4757                                                                                                        0.0000\n",
            " 59    0.4759                                                                                                        0.0000\n",
            " 60    0.4788            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 61    0.4775                                                                                                        0.0000\n",
            " 62    0.4766                                                                                                        0.0000\n",
            " 63    0.4764                                                                                                        0.0000\n",
            " 64    0.4765                                                                                                        0.0000\n",
            " 65    0.4762            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 66    0.4784                                                                                                        0.0000\n",
            " 67    0.4759                                                                                                        0.0000\n",
            " 68    0.4759                                                                                                        0.0000\n",
            " 69    0.4756                                                                                                        0.0000\n",
            " 70    0.4763            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 71    0.4778                                                                                                        0.0000\n",
            " 72    0.4756                                                                                                        0.0000\n",
            " 73    0.4758                                                                                                        0.0000\n",
            " 74    0.4756                                                                                                        0.0000\n",
            " 75    0.4761            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 76    0.4778                                                                                                        0.0000\n",
            " 77    0.4770                                                                                                        0.0000\n",
            " 78    0.4763                                                                                                        0.0000\n",
            " 79    0.4772                                                                                                        0.0000\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            " 80    0.4768            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 81    0.4784                                                                                                        0.0000\n",
            " 82    0.4767                                                                                                        0.0000\n",
            " 83    0.4772                                                                                                        0.0000\n",
            " 84    0.4761                                                                                                        0.0000\n",
            " 85    0.4759            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 86    0.4778                                                                                                        0.0000\n",
            " 87    0.4762                                                                                                        0.0000\n",
            " 88    0.4761                                                                                                        0.0000\n",
            " 89    0.4762                                                                                                        0.0000\n",
            " 90    0.4764            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 91    0.4780                                                                                                        0.0000\n",
            " 92    0.4761                                                                                                        0.0000\n",
            " 93    0.4756                                                                                                        0.0000\n",
            " 94    0.4760                                                                                                        0.0000\n",
            " 95    0.4763            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 96    0.4772                                                                                                        0.0000\n",
            " 97    0.4768                                                                                                        0.0000\n",
            " 98    0.4762                                                                                                        0.0000\n",
            " 99    0.4769            0.4237           0.4268      1.0730                                                         0.0000\n",
            "\n",
            "JAX sees the following devices: [GpuDevice(id=0, process_index=0)]\n",
            "TF sees the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "Continuing the run from the last saved checkpoint\n",
            "{'accuracy': DeviceArray(0.4263589, dtype=float32), 'nll': 1.0731524, 'ece': 0.0}\n",
            "\n",
            "python3 bnn_hmc/scripts/run_sgd.py --seed=9 --weight_decay=10 --dir=runs/sgd/mlsst/9/ --dataset_name=mlsst/Y10 --model_name=lenet --init_step_size=3e-7 --num_epochs=100 --eval_freq=5 --batch_size=100 --save_freq=5 --optimizer=SGD --image_size=64 --subset_train_to=20000 --scaling=asinh --train_split=train --test_split=validation\n",
            "JAX sees the following devices: [GpuDevice(id=0, process_index=0)]\n",
            "TF sees the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "Starting from random initialization with provided seed\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  0    4.7102            0.4237           0.4268      1.0730                                                         0.0000\n",
            "  1    0.5009                                                                                                        0.0000\n",
            "  2    0.4788                                                                                                        0.0000\n",
            "  3    0.4782                                                                                                        0.0000\n",
            "  4    0.4781                                                                                                        0.0000\n",
            "  5    0.4787            0.4237           0.4268      1.0732                                                         0.0000\n",
            "  6    0.4781                                                                                                        0.0000\n",
            "  7    0.4790                                                                                                        0.0000\n",
            "  8    0.4784                                                                                                        0.0000\n",
            "  9    0.4785                                                                                                        0.0000\n",
            " 10    0.4781            0.4237           0.4268      1.0733                                                         0.0000\n",
            " 11    0.4788                                                                                                        0.0000\n",
            " 12    0.4779                                                                                                        0.0000\n",
            " 13    0.4774                                                                                                        0.0000\n",
            " 14    0.4780                                                                                                        0.0000\n",
            " 15    0.4783            0.4239           0.4274      1.0729                                                         0.0000\n",
            " 16    0.4797                                                                                                        0.0000\n",
            " 17    0.4783                                                                                                        0.0000\n",
            " 18    0.4779                                                                                                        0.0000\n",
            " 19    0.4785                                                                                                        0.0000\n",
            " 20    0.4809            0.4236           0.4268      1.0730                                                         0.0000\n",
            " 21    0.4791                                                                                                        0.0000\n",
            " 22    0.4779                                                                                                        0.0000\n",
            " 23    0.4779                                                                                                        0.0000\n",
            " 24    0.4783                                                                                                        0.0000\n",
            " 25    0.4789            0.4238           0.4268      1.0729                                                         0.0000\n",
            " 26    0.4799                                                                                                        0.0000\n",
            " 27    0.4779                                                                                                        0.0000\n",
            " 28    0.4776                                                                                                        0.0000\n",
            " 29    0.4782                                                                                                        0.0000\n",
            " 30    0.4782            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 31    0.4797                                                                                                        0.0000\n",
            " 32    0.4789                                                                                                        0.0000\n",
            " 33    0.4788                                                                                                        0.0000\n",
            " 34    0.4790                                                                                                        0.0000\n",
            " 35    0.4787            0.4237           0.4268      1.0732                                                         0.0000\n",
            " 36    0.4786                                                                                                        0.0000\n",
            " 37    0.4779                                                                                                        0.0000\n",
            " 38    0.4779                                                                                                        0.0000\n",
            " 39    0.4778                                                                                                        0.0000\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            " 40    0.4774            0.4237           0.4268      1.0735                                                         0.0000\n",
            " 41    0.4798                                                                                                        0.0000\n",
            " 42    0.4787                                                                                                        0.0000\n",
            " 43    0.4782                                                                                                        0.0000\n",
            " 44    0.4788                                                                                                        0.0000\n",
            " 45    0.4781            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 46    0.4790                                                                                                        0.0000\n",
            " 47    0.4782                                                                                                        0.0000\n",
            " 48    0.4789                                                                                                        0.0000\n",
            " 49    0.4778                                                                                                        0.0000\n",
            " 50    0.4777            0.4237           0.4268      1.0732                                                         0.0000\n",
            " 51    0.4785                                                                                                        0.0000\n",
            " 52    0.4830                                                                                                        0.0000\n",
            " 53    0.4779                                                                                                        0.0000\n",
            " 54    0.4778                                                                                                        0.0000\n",
            " 55    0.4787            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 56    0.4798                                                                                                        0.0000\n",
            " 57    0.4779                                                                                                        0.0000\n",
            " 58    0.4788                                                                                                        0.0000\n",
            " 59    0.4785                                                                                                        0.0000\n",
            " 60    0.4774            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 61    0.4793                                                                                                        0.0000\n",
            " 62    0.4783                                                                                                        0.0000\n",
            " 63    0.4781                                                                                                        0.0000\n",
            " 64    0.4784                                                                                                        0.0000\n",
            " 65    0.4787            0.4237           0.4268      1.0732                                                         0.0000\n",
            " 66    0.4789                                                                                                        0.0000\n",
            " 67    0.4784                                                                                                        0.0000\n",
            " 68    0.4792                                                                                                        0.0000\n",
            " 69    0.4787                                                                                                        0.0000\n",
            " 70    0.4784            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 71    0.4798                                                                                                        0.0000\n",
            " 72    0.4791                                                                                                        0.0000\n",
            " 73    0.4782                                                                                                        0.0000\n",
            " 74    0.4784                                                                                                        0.0000\n",
            " 75    0.4789            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 76    0.4807                                                                                                        0.0000\n",
            " 77    0.4786                                                                                                        0.0000\n",
            " 78    0.4787                                                                                                        0.0000\n",
            " 79    0.4776                                                                                                        0.0000\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            "  i         t    train/accuracy    test/accuracy    test/nll  test/ens_accuracy    test/ens_nll    test/ens_ece          lr\n",
            "---  --------  ----------------  ---------------  ----------  -------------------  --------------  --------------  --------\n",
            " 80    0.4779            0.4237           0.4268      1.0731                                                         0.0000\n",
            " 81    0.4794                                                                                                        0.0000\n",
            " 82    0.4793                                                                                                        0.0000\n",
            " 83    0.4790                                                                                                        0.0000\n",
            " 84    0.4791                                                                                                        0.0000\n",
            " 85    0.4789            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 86    0.4797                                                                                                        0.0000\n",
            " 87    0.4785                                                                                                        0.0000\n",
            " 88    0.4780                                                                                                        0.0000\n",
            " 89    0.4782                                                                                                        0.0000\n",
            " 90    0.4782            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 91    0.4783                                                                                                        0.0000\n",
            " 92    0.4782                                                                                                        0.0000\n",
            " 93    0.4792                                                                                                        0.0000\n",
            " 94    0.4786                                                                                                        0.0000\n",
            " 95    0.4784            0.4237           0.4268      1.0730                                                         0.0000\n",
            " 96    0.4793                                                                                                        0.0000\n",
            " 97    0.4784                                                                                                        0.0000\n",
            " 98    0.4780                                                                                                        0.0000\n",
            " 99    0.4778            0.4237           0.4268      1.0730                                                         0.0000\n",
            "\n",
            "JAX sees the following devices: [GpuDevice(id=0, process_index=0)]\n",
            "TF sees the following devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "Continuing the run from the last saved checkpoint\n",
            "{'accuracy': DeviceArray(0.4263589, dtype=float32), 'nll': 1.0731369, 'ece': DeviceArray(0.00023427, dtype=float32)}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "accuracies = []\n",
        "nlls = []\n",
        "eces = []\n",
        "softmax = []\n",
        "for i, root in enumerate(glob.glob('/content/runs/sgd/mlsst/*/*/')):\n",
        "  data = np.load(root + '/test_set.npy')  \n",
        "  prediction = np.load(root + '/predictions.npy')\n",
        "  metrics = np.load(root + '/metrics.npy', allow_pickle=True)\n",
        "  accuracies.append(metrics.item()['accuracy'])\n",
        "  nlls.append(metrics.item()['nll'])\n",
        "  eces.append(metrics.item()['ece'])\n",
        "  softmax.append(np.squeeze(prediction))\n",
        "print('Accuracy', np.mean(accuracies), np.std(accuracies))\n",
        "print('NLL', np.mean(nlls), np.std(nlls))\n",
        "print('ECE', np.mean(eces), np.std(eces))\n",
        "softmax = np.array(softmax)"
      ],
      "metadata": {
        "id": "A8AgV7nTsiel",
        "outputId": "71913e1e-1244-4303-a2d6-62fecece8c8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.45630676 0.08984362\n",
            "NLL 1.2501459 0.53098905\n",
            "ECE 0.020521248175646177 0.061485694517200845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_softmax = np.mean(softmax, axis=0)\n",
        "ensemble_accuracy = np.mean(np.argmax(mean_softmax, axis=-1) == data)\n",
        "print('Ensemble accuracy', ensemble_accuracy)"
      ],
      "metadata": {
        "id": "FdFGtTTfsk2l",
        "outputId": "ef1b3416-4278-4464-91a7-557e7ac9a7c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble accuracy 0.6053611317944899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def uncertainty(softmax):\n",
        "  # Per example softmax with shape(num_examples, num_classes)\n",
        "  predictive_entropy = 0\n",
        "  single_pass_entropy = 0\n",
        "  for i in range(softmax.shape[1]):\n",
        "    # Sum over classes\n",
        "    predictive_entropy += -np.mean(softmax[:,i])*np.log(np.mean(softmax[:,i]))\n",
        "    single_pass_entropy += -softmax[:,i]*np.log(softmax[:,i])\n",
        "  single_pass_entropy = np.mean(single_pass_entropy)\n",
        "  mutual_info = predictive_entropy - single_pass_entropy\n",
        "  return predictive_entropy, single_pass_entropy, mutual_info"
      ],
      "metadata": {
        "id": "YQviG8K8sn13"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictive_entropies = [] \n",
        "single_pass_entropies = []\n",
        "mutual_infos = []\n",
        "for i in range(softmax.shape[1]):\n",
        "  predictive_entropy, single_pass_entropy, mutual_info = uncertainty(softmax[:,i,:])\n",
        "  predictive_entropies.append(predictive_entropy)\n",
        "  single_pass_entropies.append(single_pass_entropy)\n",
        "  mutual_infos.append(mutual_info)\n",
        "print('Predictive entropy:', np.mean(predictive_entropies), np.std(predictive_entropies))\n",
        "print('Single pass entropy:', np.mean(single_pass_entropies), np.std(single_pass_entropies))\n",
        "print('Mutual info:', np.mean(mutual_infos), np.std(mutual_infos))"
      ],
      "metadata": {
        "id": "Vh8vInbVsq3g",
        "outputId": "e1173628-3358-492f-f1b9-03b423e53654",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictive entropy: 1.0655909105724837 0.01808269768946763\n",
            "Single pass entropy: 0.9762528 0.020161558\n",
            "Mutual info: 0.08933810373783467 0.024365342215464537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(predictive_entropies)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d7tB5BdqsswO",
        "outputId": "c92a79e0-cdd2-4aab-dbc6-7ebeab768c28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(single_pass_entropies)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "soGV3QGIsvDz",
        "outputId": "ba488138-ac35-44ed-8fe0-1a8269df4e3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(mutual_infos)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "luKZWL3Jsw-l",
        "outputId": "e6c41d2f-c855-407f-f96a-19e06cf5c945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EyafaGXtszBF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
